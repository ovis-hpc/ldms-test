#!/usr/bin/python3
#
# README
# ------
# Test the LDMS msg on rails with `ldmsd`.
#
# - The new `ldmsd` shall be able to consume the old `ldmsd_stream` (from older
#   version of `ldmsd`) and forward the stream data if needed.
# - The new `ldmsd` shall be able to `prdcr_subscribe` to the old ldmsds with
#   exact name macthing stream.
# - The new `ldmsd` shall be able to `prdcr_subscribe` to the new ldmsds with
#   either ldmsd_stream or regular expression message channel name matching.
#
# Connectivity
#
#       pysub@agg-2
#            ^
#            |
#        ldmsd@agg-2
#            ^
#            |
#        ldmsd@agg-1
#            ^
#            |
#     .------+------.
#     |             |
# ldmsd@old     ldmsd@new <--------.
#     ^                            |
#     |                            |
# ldmsd_stream_publish@old    pypub@new

import os
import io
import re
import pwd
import pdb
import sys
import json
import time
import atexit
import argparse
import TADA
import logging

from distutils.spawn import find_executable
from LDMS_Test import LDMSDCluster, LDMSDContainer, process_args, \
                      add_common_args, jprint, parse_ldms_ls, PyPty, \
                      MsgData, LdmsAddr

if __name__ != "__main__":
    raise RuntimeError("This should not be impoarted as a module.")

class Debug(object):
    pass
D = Debug()

logging.basicConfig(format = "%(asctime)s %(name)s %(levelname)s %(message)s",
                    level = logging.INFO)

log = logging.getLogger(__name__)

exec(open(os.getenv("PYTHONSTARTUP", "/dev/null")).read())

#### default values #### ---------------------------------------------
sbin_ldmsd = find_executable("ldmsd")
if sbin_ldmsd:
    default_prefix, a, b = sbin_ldmsd.rsplit('/', 2)
else:
    default_prefix = "/opt/ovis"

#### argument parsing #### -------------------------------------------
ap = argparse.ArgumentParser(description = "Test ldms_msg features with ldmsd")
add_common_args(ap)
args = ap.parse_args()
process_args(args)

#### config variables #### ------------------------------
USER = args.user
PREFIX = args.prefix
COMMIT_ID = args.commit_id
SRC = args.src
CLUSTERNAME = args.clustername
DB = args.data_root
LDMSD_PORT = 10000

#### spec #### -------------------------------------------------------

def munge_key(i):
    return '0'*1024

common_plugin_config = [
        "component_id=%component_id%",
        "instance=%hostname%/%plugin%",
        "producer=%hostname%",
    ]
base_daemons = [
        {
            "name" : "munged",
            "type" : "munged",
            "key"  : munge_key(0),
        },
    ]
spec = {
    "name" : CLUSTERNAME,
    "description" : "{}'s ldms_msg test cluster".format(USER),
    "type" : "NA",
    "templates" : { # generic template can apply to any object by "!extends"
        "ldmsd_daemon" : {
            "name" : "ldmsd",
            "type" : "ldmsd",
            "listen" : [
                { "port" : LDMSD_PORT, "xprt" : "sock" },
            ],
        },
        "samp_daemon" : {
            "!extends" : "ldmsd_daemon",
            "samplers" : [
                {
                    "plugin" : "meminfo",
                    "interval" : 1000000,
                    "offset" : 0,
                    "config" : common_plugin_config,
                    "start" : True,
                },
            ],
        },
        "compute_node" : {
            "daemons" : base_daemons + [
                { "!extends" : "ldmsd_daemon" },
            ],
        },
    }, # templates
    "nodes" : [
        {
            "hostname" : f"agg-2",
            "daemons" : base_daemons + [
                {
                    "!extends" : "ldmsd_daemon",
                    "offset" : 400000,
                    "prdcrs" : [
                        {
                            "name" : "agg-1",
                            "host" : "agg-1",
                            "port" : LDMSD_PORT,
                            "xprt" : "sock",
                            "type" : "active",
                            "interval" : 1000000,
                        }
                    ],
                    "config" : [ # additional config applied after prdcrs
                        "load name=stream_dump",
                        "config name=stream_dump op=msg_subscribe " \
                            "message_channel=.* path=/db/%hostname%-new.txt",
                        "config name=stream_dump op=stream_subscribe " \
                            "stream=old path=/db/%hostname%-old.txt",

                        "prdcr_subscribe regex=agg-1 stream=old",
                        "prdcr_subscribe regex=agg-1 message_channel=.*",
                        "prdcr_start_regex regex=.*",
                        "updtr_add name=all interval=1000000 offset=%offset%",
                        "updtr_prdcr_add name=all regex=.*",
                        "updtr_start name=all",
                    ],
                },
            ],
        },
        {
            "hostname" : f"agg-1",
            "daemons" : base_daemons + [
                {
                    "!extends" : "ldmsd_daemon",
                    "offset" : 200000,
                    "prdcrs" : [
                        {
                            "name" : f"{h}",
                            "host" : f"{h}",
                            "port" : LDMSD_PORT,
                            "xprt" : "sock",
                            "type" : "active",
                            "interval" : 1000000,
                        } for h in [ "new", "old" ]
                    ],
                    "config" : [ # additional config applied after prdcrs
                        "load name=stream_dump",
                        "config name=stream_dump op=msg_subscribe " \
                            "message_channel=.* path=/db/%hostname%-new.txt",
                        "config name=stream_dump op=stream_subscribe " \
                            "stream=old path=/db/%hostname%-old.txt",
                        "prdcr_subscribe regex=new message_channel=new.*",
                        "prdcr_subscribe regex=old stream=old",
                        "prdcr_start_regex regex=.*",
                        "updtr_add name=all interval=1000000 offset=%offset%",
                        "updtr_prdcr_add name=all regex=.*",
                        "updtr_start name=all",
                    ],
                },
            ],
        },
        {
            "hostname" : f"new",
            "!extends" : "compute_node",
        },
        {
            "hostname" : f"old",
            "image" : "ovishpc/ldms-samp:4.3.10",
            "daemons" : base_daemons + [
                {
                    "!extends" : "ldmsd_daemon",
                    # This is a hack to supress `stream_enable` and `msg_enable`
                    # commands in old ldmsd.
                    "msg_enable" : False,
                    "stream_enable" : False,
                },
            ],
            "mounts" : [
                # mounts override. /opt/ovis won't be mounted.
                # We want to use ldmsd v4.3.10 that comes with the image.
                "{}:/db:rw".format(DB),
            ],
        },
    ], # nodes

    "cap_add": [ "SYS_PTRACE", "SYS_ADMIN" ],
    "image": args.image,
    "ovis_prefix": PREFIX,
    "env" : {
        "FOO": "BAR" ,
        "LDMS_MSG_STATS_LEVEL" : "2",
    },
    "mounts": [
        "{}:/db:rw".format(DB),
        "{0}:{1}:ro".format(os.path.realpath(sys.path[0]), "/tada-src"),
    ] + args.mount +
    ( ["{0}:{0}:ro".format(SRC)] if SRC else [] ),
}

#### helper functions ####

def EXPECT(val, expected):
    if val != expected:
        raise RuntimeError("\n  EXPECTING: {}\n  GOT: {}".format(expected, val))

#### test definition ####

test = TADA.Test(test_suite = "LDMSD",
                 test_type = "FVT",
                 test_name = "ldmsd_msg_test",
                 test_desc = "Test ldms_msg (on rails) with ldmsd",
                 test_user = args.user,
                 commit_id = COMMIT_ID,
                 tada_addr = args.tada_addr)

test.add_assertion( 1, "Check data from old ldmsd_stream at agg-1")
test.add_assertion( 2, "Check data from old ldmsd_stream at agg-2")
test.add_assertion( 3, "Check data from old ldmsd_stream at the last subscriber")

test.add_assertion( 4, "Check data from the matching new ldms msg at agg-1")
test.add_assertion( 5, "Check data from the matching new ldms msg at agg-2")
test.add_assertion( 6, "Check data from the matching new ldms msg at the last subscriber")

test.add_assertion( 7, "Check data from the non-matching new ldms stream at agg-1")
test.add_assertion( 8, "Check data from the non-matching new ldms stream at agg-2")
test.add_assertion( 9, "Check data from the non-matching new ldms stream at last subscriber")

test.add_assertion(10, "Check msg_stats before stream data transfer")
test.add_assertion(11, "Check msg_client_stats before stream data transfer")
test.add_assertion(12, "Check msg_stats after stream data transfer")
test.add_assertion(13, "Check msg_client_stats after stream data transfer")

test.add_assertion(14, "Publishing completed with a status after msg_disable")
test.add_assertion(15, "No data is propagated after msg_disable")
test.add_assertion(16, "Publisher synchronously failed afterward")
test.add_assertion(17, "New publisher synchronously failed")

# ===== Helping functions =====
def get_msg_stats(node):
    cmd = f"echo msg_stats json=1 | ldmsd_controller --xprt sock --port 10000 --host localhost"
    rc, out = node.exec_run(['/bin/bash', '-c', cmd])
    if rc:
        raise RuntimeError(f"error rc: {rc}, out: {out}")
    objs = json.loads(out)
    return objs

def get_msg_client_stats(node):
    cmd = f"echo msg_client_stats json=1 | ldmsd_controller --xprt sock --port 10000 --host localhost"
    rc, out = node.exec_run(['/bin/bash', '-c', cmd])
    if rc:
        raise RuntimeError(f"error rc: {rc}, out: {out}")
    objs = json.loads(out)
    return objs
# --------------------- #

cluster = None
test.start()

@atexit.register
def at_exit():
    rc = test.finish()
    if cluster is not None:
        cluster.remove()
    os._exit(rc)

log.info("-- Get or create the cluster --")
cluster = LDMSDCluster.get(spec["name"], create = True, spec = spec)

agg2 = cluster.get_container("agg-2")
agg1 = cluster.get_container("agg-1")
new = cluster.get_container("new")
old = cluster.get_container("old")
nodes = [ agg2, agg1, new, old ]

log.info("-- Start daemons --")
cluster.start_daemons()

log.info("... wait a bit to make sure ldmsd's are up")
time.sleep(2)

pypub = PyPty(new, "/tada-src/python/pypubsub.py")
pysub = PyPty(agg2, "/tada-src/python/pypubsub.py")

# subscirbe all streams from agg-2
pysub.cmd("r.msg_subscribe('.*', True)")

# prep 'old' stream data
old.exec_run("""echo '{"name":"data"}' > /db/data.json""")

ss0 = get_msg_stats(agg2)
cs0 = get_msg_client_stats(agg2)

# publish 'old' stream
old.exec_run("ldmsd_stream_publish -x sock -h localhost -p 10000 -s old -t string -f /db/data.json")
old.exec_run("ldmsd_stream_publish -x sock -h localhost -p 10000 -s old -t json -f /db/data.json")

# publish 'foo' msg; should not reach agg-1
pypub.cmd("r.msg_publish('foo', 'FOOFOO')")

# publish 'new' message
pypub.cmd("r.msg_publish('new_string', 'some string')")
pypub.cmd("r.msg_publish('new_json', {'new':'json'})")

# pause a little, making sure that the data all arrive at the final subscriber
time.sleep(2)

subdata = [ pysub.cmd("scli.get_data()") for i in range(0, 5) ]
subobjs = [ eval(o) if o else None for o in subdata ]

EXPECTED_OBJS = [
        MsgData(name='new_string', is_json=0, data='some string'),
        MsgData(name='new_json', is_json=1, data={'new': 'json'}),
        None,
        None,
        None
        ]

agg1_old_dump = open(f"{DB}/agg-1-old.txt").read()
agg2_old_dump = open(f"{DB}/agg-2-old.txt").read()
agg1_new_dump = open(f"{DB}/agg-1-new.txt").read()
agg2_new_dump = open(f"{DB}/agg-2-new.txt").read()

def dump2array(dump_text):
    rec_list = dump_text.split('\x01')[1:] # skip the first empty record
    return [ r.split(': ', 1)[1] for r in rec_list ]

EXPECTED_RECS = [ '{"name":"data"}\n\n' ,
                  '{"name":"data"}\n\n' ,
                  'some string\n'       ,
                  '{"new": "json"}\n'     ]

agg1_old_recs = dump2array(agg1_old_dump)
agg2_old_recs = dump2array(agg2_old_dump)
agg1_new_recs = dump2array(agg1_new_dump)
agg2_new_recs = dump2array(agg2_new_dump)

agg1_recs = agg1_old_recs + agg1_new_recs
agg2_recs = agg2_old_recs + agg2_new_recs

# For old ldmsd_stream
_sli = slice(0, 2)
a1_recs = agg1_recs[_sli]
a2_recs = agg2_recs[_sli]
ERECS   = EXPECTED_RECS[_sli]
test.assert_test(1, a1_recs == ERECS, "")
test.assert_test(2, a2_recs == ERECS, "")

# The old stream shall not reach the new subscriber
for o in subobjs:
    if o and 'old' in o.name:
        # found old stream ...
        test.assert_test(3, False, "Found 'old' stream in the new subscriber, obj: {o}")
        break
else:
    # no old stream found
    test.assert_test(3, True, "")

# For the new messages
_sli = slice(2, 4)
a1_recs = agg1_recs[_sli]
a2_recs = agg2_recs[_sli]
ERECS   = EXPECTED_RECS[_sli]
test.assert_test(4, a1_recs == ERECS, "")
test.assert_test(5, a2_recs == ERECS, "")

test.assert_test(6, subobjs == EXPECTED_OBJS, "")

# For the non-matching msgs
test.assert_test(7, len(agg1_recs) == 4, "")
test.assert_test(8, len(agg2_recs) == 4, "")
test.assert_test(9, subobjs[4:] == EXPECTED_OBJS[4:], "")

ss1 = get_msg_stats(agg2)
cs1 = get_msg_client_stats(agg2)

# test.add_assertion(10, "Check msg_stats before stream data transfer")
test.assert_test(10, ss0 == [], "")

CLIENT_DESCS = set(["stream_dump, path:/db/agg-2-new.txt", "remote_client"])

# test.add_assertion(11, "Check msg_client_stats before stream data transfer")
while True: # will break
    if len(cs0) != 2:
        test.assert_test(11, False, f"bad msg_client_stats data: {cs0}")
        break
    break0 = True
    for s in cs0:
        if s['tx']['bytes'] != 0 or s['drops']['bytes'] != 0 \
                or s['tx']['count'] != 0 or s['drops']['count'] != 0:
            test.assert_test(11, False, f"bad msg_client_stats data: {cs0}")
            break
    else:
        break0 = False
    if break0:
        break
    client_descs = set(o['desc'] for o in cs0)
    if client_descs != CLIENT_DESCS:
        test.assert_test(11, False, f"bad msg_client_stats data: {cs0}")
        break
    test.assert_test(11, True, "")
    break

# test.add_assertion(12, "Check msg_stats after stream data transfer")
while True: # will break
    EXPECTED_NAMES = set(["new_json", "new_string"])
    names = set([ o["name"] for o in ss1])
    if names != EXPECTED_NAMES:
        test.assert_test(12, False, f"bad msg_stats data: {ss1}")
        break
    break0 = True
    for o in ss1:
        descs = set( e["client_desc"] for e in o["clients"] )
        if descs != CLIENT_DESCS:
            test.assert_test(12, False, f"bad msg_stats data: {ss1}")
            break
        srcs = [ (v['bytes'], v['count']) for v in o['sources'].values() ]
        EXPECTED = [ (o['rx']['bytes'], o['rx']['count']) ]
        if srcs != EXPECTED:
            test.assert_test(12, False, f"bad msg_stats data: {ss1}")
            break
        break1 = True
        for c in o['clients']:
            tx = [ (c['tx']['bytes'], c['tx']['count']) ]
            if tx != EXPECTED:
                test.assert_test(12, False, f"bad msg_stats data: {ss1}")
                break
        else:
            break1 = False
        if break1:
            break
    else:
        break0 = False
    if break0:
        break
    # check stream stats
    data = set( (o['name'], o['rx']['bytes'], o['rx']['count']) for o in ss1 )
    EXPECTED = set([
                ("new_json",   15, 1),
                ("new_string", 11, 1),
                ])
    if data != EXPECTED:
        test.assert_test(12, False, f"bad msg_stats data: {ss1}")
        break
    test.assert_test(12, True, "")
    break

def dict_filter(d, keys):
    return { k:d[k] for k in keys }

def client_xtract(c):
    obj = dict()
    obj['tx'] = dict_filter(c['tx'], ['bytes', 'count'])
    obj['drops'] = dict_filter(c['drops'], ['bytes', 'count'])
    channels = list()
    for s in c['channels']:
        _s = dict()
        _s['name'] = s['name']
        _s['tx'] = dict_filter(s['tx'], ['bytes', 'count'])
        _s['drops'] = dict_filter(s['drops'], ['bytes', 'count'])
        channels.append(_s)
    channels.sort(key = lambda o: o['name'])
    obj['channels'] = channels
    return obj

# test.add_assertion(13, "Check msg_client_stats after stream data transfer")
while True: # will break
    descs = set( o['desc'] for o in cs1 )
    if descs != CLIENT_DESCS:
        test.assert_test(13, False, f"bad msg_client_stats data: {cs1}")
        break
    # only 2 clients with same stats
    cx0 = client_xtract(cs1[0])
    cx1 = client_xtract(cs1[1])
    EXPECTED = {
            'tx': {'bytes': 26, 'count': 2},
            'drops': {'bytes': 0, 'count': 0},
            'channels': [
                {
                    'name': 'new_json',
                    'tx': {'bytes': 15, 'count': 1},
                    'drops': {'bytes': 0, 'count': 0},
                },
                {
                    'name': 'new_string',
                    'tx': {'bytes': 11, 'count': 1},
                    'drops': {'bytes': 0, 'count': 0},
                },
            ]
        }
    if cx0 != EXPECTED:
        test.assert_test(13, False, f"bad msg_client_stats data: {cx0}")
        break
    if cx1 != EXPECTED:
        test.assert_test(13, False, f"bad msg_client_stats data: {cx1}")
        break
    test.assert_test(13, True, "")
    break

new.config_ldmsd("msg_disable")
empty = pypub.cmd("r.msg_publish('new_string', 'block-me')")
time.sleep(2)


# test.add_assertion(14, "Publishing completed with a status after msg_disable")
out = pypub.cmd("events[-1].quota.rc")
qrc = int(out)
test.assert_test(14, qrc == 95, "")

# test.add_assertion(15, "No data is propagated after msg_disable")
d_agg1_old_dump = open(f"{DB}/agg-1-old.txt").read()
d_agg2_old_dump = open(f"{DB}/agg-2-old.txt").read()
d_agg1_new_dump = open(f"{DB}/agg-1-new.txt").read()
d_agg2_new_dump = open(f"{DB}/agg-2-new.txt").read()
d = [ d_agg1_old_dump, d_agg2_old_dump, d_agg1_new_dump, d_agg2_new_dump ]
e = [ agg1_old_dump, agg2_old_dump, agg1_new_dump, agg2_new_dump ]
test.assert_test(15, d == e, "")

# test.add_assertion(16, "Publisher synchronously failed afterward")
out = pypub.cmd("r.msg_publish('new_string', 'block-me')")
test.assert_test(16, 0 < out.find("rc: 95"), "")

# test.add_assertion(17, "New publisher synchronously failed")
newpub = PyPty(new, "/tada-src/python/pypubsub.py")
out = newpub.cmd("r.msg_publish('new_string', 'block-me')")
test.assert_test(17, 0 < out.find("rc: 95"), "")

# see at_exit()
