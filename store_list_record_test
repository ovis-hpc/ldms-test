#!/usr/bin/env python3
#
# Test store_sos storing lists

import argparse
import atexit
import json
import logging
import numpy
import os
import random
import sys
import TADA

from collections import namedtuple
from distutils.spawn import find_executable
from itertools import combinations_with_replacement
from time import sleep

from LDMS_Test import LDMSDCluster, LDMSDContainer, process_args, \
                      add_common_args, assertion_id_get

if __name__ != "__main__":
    raise RuntimeError("This should not be impoarted as a module.")

class Debug(object):
    pass
D = Debug()

logging.basicConfig(format = "%(asctime)s %(name)s %(levelname)s %(message)s",
                    level = logging.INFO)

log = logging.getLogger(__name__)

exec(open(os.getenv("PYTHONSTARTUP", "/dev/null")).read())

#### default values #### ---------------------------------------------
sbin_ldmsd = find_executable("ldmsd")
if sbin_ldmsd:
    default_prefix, a, b = sbin_ldmsd.rsplit('/', 2)
else:
    default_prefix = "/opt/ovis"

#### argument parsing #### -------------------------------------------
ap = argparse.ArgumentParser(description = "Test store_sos storing lists")
add_common_args(ap)
args = ap.parse_args()
process_args(args)

PREFIX = args.prefix
CLUSTERNAME = args.clustername
USER = args.user

#### config variables #### ------------------------------
LDMSD_PORT = 10001
LDMSD_XPRT = "sock"
STORE_ROOT = "/store" # path inside container (agg-2)
SET_ARRAY_CARD = 10
SAMP_INTERVAL = 1000000
SAMP_OFFSET = 0
RECONNECT_TIME = 5000000
UPDATE_INTERVAL = 1000000
UPDATE_OFFSET = 100000
STORE_SOS_PATH = STORE_ROOT + "/sos"
STORE_CSV_PATH = STORE_ROOT + "/csv"

VALUE_TYPES = ["record", "u64", "u64_array"]
NUM_LISTS = len(VALUE_TYPES)
COMBINATION = [list(i) for i in combinations_with_replacement(VALUE_TYPES, NUM_LISTS)]
for c in COMBINATION:
    random.shuffle(c)

def schema_from_types(vtypes):
    return vtypes if isinstance(vtypes, str) else "_".join(vtypes)

spec = {
    "name" : CLUSTERNAME,
    "description" : "{}'s store_sos_list_record_support_test cluster".format(USER),
    "type" : "NA",
    "templates" : {
        "ldmsd-base" : {
            "type" : "ldmsd",
            "listen" : [
                { "port" : LDMSD_PORT, "xprt" : LDMSD_XPRT }
            ]
        },
        "aggregator" : {
            "name" : "agg",
            "!extends" : "ldmsd-base",
            "prdcrs" : [
                {
                    "name" : "sampler",
                    "host" : "%name%",
                    "xprt" : LDMSD_XPRT,
                    "port" : LDMSD_PORT,
                    "type" : "active",
                    "interval" : RECONNECT_TIME
                }
            ],
        }
    },
    "nodes" : [
        {
            "hostname" : "sampler",
            "component_id" : 1,
            "daemons" : [
                {
                    "name" : "sshd",
                    "type" : "sshd"
                },
                {
                    "name" : "samplerd",
                    "!extends" : "ldmsd-base",
                    "config" : [
                        "load name=test_sampler",
                     ] + ["config name=test_sampler action=add_lists max_len=4 " \
                            "min_len=1 schema={schema} value_types={types}".format(
                            schema = s, types = s) for s in VALUE_TYPES
                    ] + ["config name=test_sampler action=add_lists max_len=4 " \
                            "min_len=1 schema={schema} value_types={types}".format(
                            schema = schema_from_types(s), types = ",".join(s)) 
                            for s in COMBINATION
                    ] + ["config name=test_sampler action=add_set producer=sampler " \
                            "schema={0} instance=sampler/{0}".format(s) for s in VALUE_TYPES
                    ] + ["config name=test_sampler action=add_set producer=sampler " \
                            "schema={0} instance=sampler/{0}".format(schema_from_types(s))
                            for s in COMBINATION
                    ] + [ "start name=test_sampler interval={i} offset={o}".format(
                            i = SAMP_INTERVAL, o = SAMP_OFFSET)
                    ]
                }
            ]
        },
        {
            "hostname" : "agg_sos",
            "daemons" : [
                {
                    "name" : "sshd",
                    "type" : "sshd"
                },
                {
                    "name" : "agg",
                    "!extends" : "ldmsd-base",
                    "prdcrs" : [
                        {   "name" : "sampler",
                            "host" : "%name%",
                            "xprt" : LDMSD_XPRT,
                            "port" : LDMSD_PORT,
                            "type" : "active",
                            "interval" : RECONNECT_TIME,
                        }
                    ],
                    "config" : [
                        "prdcr_start_regex regex=.*",
                        "load name=store_sos",
                        "config name=store_sos path={}".format(STORE_SOS_PATH)
                    ] + [
                        "strgp_add name={0} plugin=store_sos " \
                            "container={0} schema={0}".format(schema_from_types(s)) for s in COMBINATION
                    ] + [
                        "strgp_add name={0} plugin=store_sos " \
                            "container={0} schema={0}".format(s) for s in VALUE_TYPES
                    ] + [
                        "strgp_start name={}".format(schema_from_types(s)) for s in COMBINATION
                    ] + [
                        "strgp_start name={}".format(s) for s in VALUE_TYPES
                    ] + [
                        "updtr_add name=all interval={i} offset={o}".format(
                            i = UPDATE_INTERVAL, o = UPDATE_OFFSET),
                        "updtr_prdcr_add name=all regex=.*",
                        "updtr_start name=all"
                    ]
                }
            ]
        },
        {
            "hostname" : "agg_csv",
            "daemons" : [
                {
                    "name" : "sshd",
                    "type" : "sshd"
                },
                {
                    "name" : "agg",
                    "!extends" : "aggregator",
                    "config" : [
                        "prdcr_start_regex regex=.*",
                        "load name=store_csv",
                        "config name=store_csv path={}".format(STORE_CSV_PATH)
                    ] + [
                        "strgp_add name={0} plugin=store_csv " \
                            "container={0} schema={0}".format(schema_from_types(s)) for s in COMBINATION
                    ] + [
                        "strgp_add name={0} plugin=store_csv " \
                            "container={0} schema={0}".format(s) for s in VALUE_TYPES
                    ] + [
                        "strgp_start name={}".format(schema_from_types(s)) for s in COMBINATION
                    ] + [
                        "strgp_start name={}".format(s) for s in VALUE_TYPES
                    ] + [
                        "updtr_add name=all interval={i} offset={o}".format(
                            i = UPDATE_INTERVAL, o = UPDATE_OFFSET),
                        "updtr_prdcr_add name=all regex=.*",
                        "updtr_start name=all"
                    ]
                }
            ]
        }
    ],
    "cap_add" : ["SYS_PTRACE", "SYS_ADMIN"],
    "image" : args.image,
    "ovis_prefix" : PREFIX,
    "env" : {
             "TADA_USER" : args.user,
             "TADA_ADDR" : args.tada_addr
            },
    "mounts" : args.mount + [
                "{}:/db:rw".format(args.data_root)
                ] + (["{}:{}:ro".format(args.src)] if args.src else [])
}

def __char(round):
    return 'a' + (round%26)

def __v8(round):
    return round & 0xFF

def __v16(round):
    return round & 0xFFFF

def __v32(round):
    return round & 0xFFFFFFFF

ValueType = namedtuple('ValueType', ['sos_type', 'verify'])
PRIMITIVE_V_TYPES = {
    "LDMS_V_CHAR"       : ValueType("UINT32", lambda round, v : v == __char(round)),
    "LDMS_V_U8"         : ValueType("UINT32", lambda round, v : v == __v8(round)),
    "LDMS_V_S8"         : ValueType("INT32", lambda round, v : v == __v8(round)),
    "LDMS_V_U16"        : ValueType("UINT16", lambda round, v : v == __v16(round)),
    "LDMS_V_S16"        : ValueType("INT16", lambda round, v : v == __v16(round)),
    "LDMS_V_U32"        : ValueType("UINT32", lambda round, v : v == __v32(round)),
    "LDMS_V_S32"        : ValueType("INT32", lambda round, v : v == __v32(round)),
    "LDMS_V_U64"        : ValueType("UINT64", lambda round, v : v == round),
    "LDMS_V_S64"        : ValueType("INT64", lambda round, v : v == round),
    "LDMS_V_F32"        : ValueType("FLOAT", lambda round, v : v == round),
    "LDMS_V_D64"        : ValueType("DOUBLE", lambda round, v : v == round),
    "LDMS_V_CHAR_ARRAY" : ValueType("CHAR_ARRAY", lambda round, v : v == __char(round)*len(v)),
    "LDMS_V_U8_ARRAY"   : ValueType("BYTE_ARRAY", lambda round, v : v ==  ":".join(str(a) for a in [__v8(round)] * len(v))),
    "LDMS_V_S8_ARRAY"   : ValueType("BYTE_ARRAY", lambda round, v : v == [__v8(round)] * len(v)),
    "LDMS_V_U16_ARRAY"  : ValueType("UINT16_ARRAY", lambda round, v : v == [__v16(round)] * len(v)),
    "LDMS_V_S16_ARRAY"  : ValueType("INT16_ARRAY", lambda round, v : v == [__v16(round)] * len(v)),
    "LDMS_V_U32_ARRAY"  : ValueType("UINT32_ARRAY", lambda round, v : v == [__v32(round)] * len(v)),
    "LDMS_V_S32_ARRAY"  : ValueType("INT32_ARRAY", lambda round, v : v == [__v32(round)] * len(v)),
    "LDMS_V_U64_ARRAY"  : ValueType("UINT64_ARRAY", lambda round, v : v == [round] * len(v)),
    "LDMS_V_S64_ARRAY"  : ValueType("INT64_ARRAY", lambda round, v : v == [round] * len(v)),
    "LDMS_V_F32_ARRAY"  : ValueType("FLOAT_ARRAY", lambda round, v : v == [round] * len(v)),
    "LDMS_V_D64_ARRAY"  : ValueType("DOUBLE_ARRAY", lambda round, v : v == [round] * len(v)),
}

@atexit.register
def at_exit():
    rc = test.finish()
    if (not args.debug) and (cluster is not None):
        cluster.remove()
    os._exit(rc)

def store_sos_db_get(strgp_name):
    return "{}/{}".format(STORE_SOS_PATH, strgp_name)

def sos_query(dcont, scont, schema, idx, cond = []):
    cmd = "sos_cmd -C {} -q -S {} -X {} -f json".format(scont, schema, idx)
    for c in cond:
        cmd += " -F {}".format(c)
    rc, out = dcont.exec_run(cmd)
    if rc != 0:
        raise RuntimeError("sos_cmd error {}, out: {}".format(rc, out))
    data = json.loads(out)
    data = data['data']
    for d in data:
        d['schema'] = schema
    return data

def csv_query(dcont, schema, min_time = None):
    data = dcont.read_file(path = "{}/{}/{}".format(STORE_CSV_PATH, schema, schema))
    while 0 == len(data):
        sleep(5)
        data = dcont.read_file(path = "{}/{}/{}".format(STORE_CSV_PATH, schema, schema))
    data = data.split()
    keys = data[0][1:].split(',')
    keys = ['timestamp' if k == 'Time' else k for k in keys]
    data = [data[i] for i in range(1, len(data)) if data[i][0] != '#']
    d = [dict(zip(keys, data[i].split(','))) for i in range(len(data))]
    start_idx = 0
    if min_time:
        for l in d:
            if float(l['timestamp']) < float(min_time):
                start_idx += 1
                continue
    return d[start_idx:]

id = assertion_id_get()

SOS_AGG_RUNNING = next(id)
CSV_AGG_RUNNING = next(id)

SOS_DATABASE_CREATED = next(id)
SOS_VERIFY_DATA = next(id)
SOS_RESTART_VERIFY_DATA = next(id)

CSV_DATABASE_CREATED = next(id)
CSV_VERIFY_DATA = next(id)
CSV_RESTART_VERIFY_DATA = next(id)

def database_exist(cont, path):
    db = ["{}/{}".format(path, s) for s in [schema_from_types(k) for k in COMBINATION]]
    db += ["{}/{}".format(path, s) for s in VALUE_TYPES]
    for a in db:
        if not cont.files_exist(a, timeout = 10):
            return (False, "agg.file_exists{}".format(a))
    return (True, "agg.file_exists(a) for a in db_paths")

def is_same_sample(prev_ts, ts):
    if 0 == prev_ts:
        return True
    if prev_ts == ts:
        return True
    else:
        return False

def verify_row(o):
    round = o["round"]
    for n, v in o.items():
        if n in PRIMITIVE_V_TYPES.keys():
            if PRIMITIVE_V_TYPES[n].verify:
                is_same = PRIMITIVE_V_TYPES[n].verify(round, v)
                if not is_same:
                    return (False, round, n, v)
    return (True, None, None, None)

def verify_data(num_lists, schema, data):
    prev_ts = 0
    cnt = 0
    round0 = data[0]["round"] # First line of data
    for d in data:
        ts = d["timestamp"]
        # Skip the first round in the database
        # store_sos may store the data from the transaction partially.
        if d["round"] == round0:
            prev_ts = ts
            continue

        if not is_same_sample(prev_ts, ts):
            list_len = [int(d[k]) for k in ["list_{}_len".format(i) for i in range(1, num_lists+1)]]
            exp_lent_idx = [0] * num_lists
            prev_ts = ts
        else:
            exp_lent_idx = [lent_idx[i] if lent_idx[i] + 1 == list_len[i] else \
                            lent_idx[i] + 1 for i in range(num_lists)]

        lent_idx = [int(d[k]) for k in ["list_{}_entry_idx".format(i) for i in range(1, num_lists+1)]]
        if exp_lent_idx != lent_idx:
            return (False, "{}: store_sos skiped or doubly stored some data point. list_len {} " \
                    "expected_ent_indices == ent_indices: {} == {}".format( \
                    schema, list_len, exp_lent_idx, lent_idx))

        if 0 in set(list_len):
            raise Exception("store_sos stores a sample with an empty list. This is not supported by design.")

        good, round, typ, v = verify_row(d)
        if not good:
            return (False, "round == d[{}]: {}".format(typ, d))
    return (True, "verify_data(db) for db in all_db")

#### Test Definition ####
test = TADA.Test(test_suite = "LDMSD",
                 test_type = "FVT",
                 test_name = "store_sos_lists_test",
                 test_desc = "Test store_sos storing lists",
                 test_user = args.user,
                 commit_id = args.commit_id,
                 tada_addr = args.tada_addr,
                 )

test.add_assertion(SOS_AGG_RUNNING, "aggregator with store_sos has started properly.")
test.add_assertion(CSV_AGG_RUNNING, "aggregator with store_csv has started properly.")

test.add_assertion(SOS_DATABASE_CREATED, "store_sos is storing data.")
test.add_assertion(SOS_VERIFY_DATA, "store_sos stores data correctly.")
test.add_assertion(SOS_RESTART_VERIFY_DATA, "store_sos stores data after restarted correctly.")

test.add_assertion(CSV_DATABASE_CREATED, "store_csv is storing data.")
test.add_assertion(CSV_VERIFY_DATA, "store_csv stores data correctly.")
test.add_assertion(CSV_RESTART_VERIFY_DATA, "store_csv stores data after restarted correctly.")

log.info("-- Get or create the cluster --")

cluster = None
test.start()

cluster = LDMSDCluster.get(spec["name"], create = True, spec = spec)

smplr = cluster.get_container("sampler")
agg_sos = cluster.get_container("agg_sos")
agg_csv = cluster.get_container("agg_csv")

agg_sos.exec_run("mkdir -p {}".format(STORE_SOS_PATH))
agg_csv.exec_run("mkdir -p {}".format(STORE_CSV_PATH))

log.info("Waiting ... for all LDMSDs to start")
cluster.start_daemons()
cluster.make_known_hosts()

while True:
    if smplr.check_ldmsd() is False:
        sleep(1)
    else:
        break
log.info("All sampler daemons are up.")

# Check whether the aggregators are running or not
test.assert_test(SOS_AGG_RUNNING, agg_sos.check_ldmsd(), "agg_sos.check_ldmsd()")
test.assert_test(CSV_AGG_RUNNING, agg_csv.check_ldmsd(), "agg_csv.check_ldmsd()")

# store_sos
sleep(1)
result, reason = database_exist(agg_sos, STORE_SOS_PATH)
test.assert_test(SOS_DATABASE_CREATED, result, reason)

for value_types in (COMBINATION + VALUE_TYPES):
    num_lists = 1 if isinstance(value_types, str) else len(value_types)
    schema = schema_from_types(value_types)

    data = sos_query(agg_sos, store_sos_db_get(schema), schema, "time_job_comp")
    while 0 == len(data):
        sleep(5)
        data = sos_query(agg_sos, store_sos_db_get(schema), schema, "time_job_comp")
    good, reason = verify_data(num_lists, schema, data)
    if good is False:
        break
test.assert_test(SOS_VERIFY_DATA, good, reason)

# Randomly stop and start 3 schema
VT = random.sample(COMBINATION, 3)
for s in VT:
    agg_sos.config_ldmsd("strgp_stop name={}".format(schema_from_types(s)))

sleep(1)
rc, agg_time = agg_sos.exec_run(cmd = "date +%s")
for s in VT:
    agg_sos.config_ldmsd("strgp_start name={}".format(schema_from_types(s)))

sleep(5)
for s in VT:
    num_lists = 1 if isinstance(s, str) else len(s)
    schema = schema_from_types(s)
    data = sos_query(agg_sos, store_sos_db_get(schema), schema, "time_job_comp", cond = ["timestamp:gt:{}".format(agg_time)])
    while 0 == len(data):
        sleep(5)
        data = sos_query(agg_sos, store_sos_db_get(schema), schema, "time_job_comp", cond = ["timestamp:gt:{}".format(agg_time)])

    good, reason = verify_data(num_lists, schema, data)
    if good is False:
        break
test.assert_test(SOS_RESTART_VERIFY_DATA, good, reason)

#--------------------------------------------------------
# store csv
result, reason = database_exist(agg_csv, STORE_CSV_PATH)
test.assert_test(CSV_DATABASE_CREATED, result, reason)

sleep(5)
for value_types in (COMBINATION + VALUE_TYPES):
    num_lists = 1 if isinstance(value_types, str) else len(value_types)
    schema = schema_from_types(value_types)
    # Stop store_csv so that it would flush the data
    agg_csv.config_ldmsd("strgp_stop name={}".format(schema))

    data = csv_query(agg_csv, schema)
    good, reason = verify_data(num_lists, schema, data)
    if good is False:
        break
test.assert_test(CSV_VERIFY_DATA, good, reason)

# Randomly stop and start 3 schema
VT = random.sample(COMBINATION, 3)
sleep(1)
rc, agg_time = agg_csv.exec_run(cmd = "date +%s")
agg_time = agg_time.strip()
for s in VT:
    agg_csv.config_ldmsd("strgp_start name={}".format(schema_from_types(s)))

sleep(5)
for s in VT:
    # Stop store_csv so that it would flush the data
    agg_csv.config_ldmsd("strgp_stop name={}".format(schema_from_types(s)))

sleep(1)
for s in VT:
    num_lists = 1 if isinstance(s, str) else len(s)
    schema = schema_from_types(s)
    data = csv_query(agg_csv, schema, min_time = agg_time)
    good, reason = verify_data(num_lists, schema, data)
    if good is False:
        break
test.assert_test(CSV_RESTART_VERIFY_DATA, good, reason)

