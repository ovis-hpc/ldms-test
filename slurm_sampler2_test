#!/usr/bin/env python3

import argparse
import atexit
import json
import logging
import os
import sys
import TADA

from distutils.spawn import find_executable
from LDMS_Test import LDMSDCluster, LDMSDContainer, process_args, \
                      add_common_args, parse_ldms_ls, \
                      assertion_id_get
from time import sleep, time

if __name__ != "__main__":
    raise RuntimeError("This should not be imported as a module")

class Debug(object):
    pass
D = Debug()

logging.basicConfig(format = "%(asctime)s %(name)s %(levelname)s %(message)s",
                    level = logging.INFO)

log = logging.getLogger(__name__)

exec(open(os.getenv("PYTHONSTARTUP", "/dev/null")).read())

SCRIPT_DIR = os.path.realpath(sys.path[0])

#### default values ####
sbin_ldmsd = find_executable("ldmsd")
if sbin_ldmsd:
    default_prefix, a, b = sbin_ldmsd.rsplit('/', 2)
else:
    default_prefix = "/opt/ovis"

#### argument parsing ####
ap = argparse.ArgumentParser(description = "Run test against the slurm_sampler2 plugin")
add_common_args(ap)
ap.add_argument("--slurm-notifier", type = str,
                default = "__find_from_prefix__",
                help = "The path (in container) to slurm_notifier library")
ap.add_argument("--num-compute", type = int,
                default = 2,
                help = "Number of compute nodes/")
args = ap.parse_args()
process_args(args)

#### config variables ####
LDMSD_XPRT = "sock"
LDMSD_PORT = "10001"
USERS = {"user1": 1001 }
CPU_PER_NODE = 2
DELETE_JOB_TIME = 2

STORE_ROOT = "/store"
STREAM_DATA_FILE = "slurm_stream.json"

SLURM_NOTIFIER = args.slurm_notifier
if SLURM_NOTIFIER == "__find_from_prefix__":
    paths = map(lambda x: f"{args.prefix}/{x}/ovis-ldms/libslurm_notifier.so",
                ["lib", "lib64"])
    for p in paths:
        if os.path.exists(p):
            SLURM_NOTIFIER = p.replace(args.prefix, '/opt/ovis', 1)
            break
    else:
        raise RuntimeError("libslurm_notifier.so not found")

#### Test specification ####
common_sampler_config = [
                            "component_id=%component_id%",
                            "instance=%hostname%/%plugin%",
                            "producer=%hostname%",
                        ]
common_daemons = [
                    {
                        "name" : "sshd",
                        "type" : "sshd"
                    },
                    {
                        "name" : "slurmd",
                        "!extends" : "slurmd",
                    },
]
spec = {
    "name" : args.clustername,
    "description" : f"{args.user}'s slurm_sampler2 test cluster",
    "type" : "NA",
    "templates" : {
        "slurmd" : {
            "name" : "slurmd",
            "type" : "slurmd",
            "plugstack" : [
                {
                    "required" : True,
                    "path" : SLURM_NOTIFIER,
                    "args" : [
                        "auth=none",
                        f"port={LDMSD_PORT}",
                        f"client={LDMSD_XPRT}:localhost:{LDMSD_PORT}:none"
                    ]
                }
            ]
        },
        "ldmsd-base" : {
            "type" : "ldmsd",
            "listen" : [
                { "port" : LDMSD_PORT, "xprt" : LDMSD_XPRT }
            ],
            "log_level" : "DEBUG"
        },
        "slurm_sampler2" : {
            "plugin" : "slurm_sampler2",
            "config" : common_sampler_config
        },
        "prdcr" : {
            "host" : "%name%",
            "port" : LDMSD_PORT,
            "xprt" : LDMSD_XPRT,
            "type" : "active",
            "interval" : 1000000,
        }
    }, # Templates
    "nodes" : [
        {
            "hostname" : "node-1",
            "component_id" : 10001,
            "daemons" : common_daemons + [
                {
                    "name" : "sampler-daemon",
                    "!extends" : "ldmsd-base",
                    "samplers" : [
                        {
                            "plugin" : "slurm_sampler2",
                            "config" : common_sampler_config + [
                                "delete_time=-1"
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "hostname" : "node-2",
            "component_id" : 10002,
            "daemons" : common_daemons + [
                {
                    "name" : "sampler-daemon",
                    "!extends" : "ldmsd-base",
                    "samplers" : [
                        {
                            "plugin" : "slurm_sampler2",
                            "config" : common_sampler_config + [
                                "job_count=2",
                                "task_count=3",
                                "delete_time=-1"
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "hostname" : "node-3",
            "component_id" : 10003,
            "daemons" : common_daemons + [
                {
                    "name" : "sampler-daemon",
                    "!extends" : "ldmsd-base",
                    "samplers" : [
                        {
                            "plugin" : "slurm_sampler2",
                            "config" : common_sampler_config + [
                                f"delete_time={DELETE_JOB_TIME}" # delete any jobs completed longer than 5 seconds
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "hostname" : "node-4",
            "component_id" : 10004,
            "daemons" : common_daemons + [
                {
                    "name" : "sampler-daemon",
                    "!extends" : "ldmsd-base",
                    "samplers" : [
                        {
                            "plugin" : "slurm_sampler2",
                            "config" : common_sampler_config + [
                                "delete_time=0" # Delete any completed jobs when the plugin receives a new job data.
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "hostname" : "agg",
            "daemons" : [
                {
                    "name" : "sshd",
                    "type" : "sshd"
                },
                {
                    "name" : "agg",
                    "!extends" : "ldmsd-base",
                    "config" : [
                        "prdcr_start_regex regex=.*",
                        "updtr_add name=all interval=1000000 offset=100000 push=onchange",
                        "updtr_prdcr_add name=all regex=.*",
                        "updtr_start name=all"
                    ],
                    "prdcrs" : [
                        {
                            "name" : f"node-{i}",
                            "!extends" : "prdcr"
                        } for i in range(1, 4)
                    ]
                }
            ]
        },
        {
            "hostname" : "headnode",
            "daemons" : [
                {
                    "name" : "sshd",
                    "type" : "sshd"
                },
                {
                    "name" : "slurmctld",
                    "type" : "slurmctld"
                }
            ]
        }
    ], # nodes

    "cpu_per_node" : CPU_PER_NODE,
    "oversubscribe" : "FORCE",
    "cap_add" : [ "SYS_PTRACE", "SYS_ADMIN" ],
    "image" : args.image,
    "ovis_prefix" : args.prefix,
    "mounts" :  [
                    f"{args.data_root}:/db:rw",
                    f"{os.path.realpath(sys.path[0])}:/tada-src/:ro",
                ] + args.mount +
                ( [f"{args.src}:{args.src}:ro"] if args.src else [] )
}

#### Clean up db ####
def cleanup_db(cluster):
    cont = cluster.get_container("headnode")
    LST = [ "job.sh", "prog", "prog.c", "slurm*.out", f"*{STREAM_DATA_FILE}*"]
    LST = [ "/db/{}".format(x) for x in LST ]
    LST += [ "{}/{}".format(STORE_ROOT, x) for x in [ "slurm" ] ]
    cont.exec_run("rm -fr {}".format(" ".join(LST)))

EXIT_RC = -1

JOB_STATE_FREE = 0
JOB_STATE_STARTING = 1
JOB_STATE_RUNNING = 2
JOB_STATE_STOPPING = 3
JOB_STATE_COMPLETE = 4

SET_DEFAULT = {'component_id': None,
               'job_data' : 'REC_DEF',
               'task_data' : 'REC_DEF',
               'job_list' : None,
               'task_list' : None}

STEPD_EVENT = ["job_init", "step_init", "task_init", "task_exit", "job_exit"]

JOB_RECORD_DEF = ["job_id", "app_id", "user", "job_name", "job_tag", "job_state",
                  "job_size", "job_uid", "job_gid",
                  "job_start", "job_end",
                  "node_count", "task_count"]
JOB_RECORD_DEFAULT_V = dict.fromkeys(JOB_RECORD_DEF, '')
JOB_RECORD_DEFAULT_V['app_id'] = 0
JOB_RECORD_DEFAULT_V['job_uid'] = 0
JOB_RECORD_DEFAULT_V['job_gid'] = 0
JOB_RECORD_DEFAULT_V['node_count'] = 0
TASK_RECORD_DEF = ["job_id", "task_pid", "task_rank", "task_exit_status"]
TASK_RECORD_DEFAULT_V = dict.fromkeys(TASK_RECORD_DEF, 0)

# If 'value' is not None, the expected metric value is the value of 'value'
# If 'value' is None, the expected metric value is the value of the Job/Task <'attr'> attribute.
# if both 'attr' and 'value' are None, the test ignores the metrics.
UPDATED_METRICS = { 'job_init' : { 'job': { 'job_id'      : { 'attr' : "job_id", 'value' : None },
                                            'job_uid'     : { 'attr' : "uid", 'value' : None },
                                            'job_gid'     : { 'attr' : "gid", 'value' : None },
                                            'job_size'    : { 'attr': "total_tasks", 'value' : None },
                                            'task_count'  : { 'attr' : "local_tasks", 'value' : None },
                                            'job_start'   : { 'attr' : None, 'value' : None }, # skip
                                            'job_end'   : { 'attr' : None, 'value' : None }, # skip
                                            'job_state'   : { 'attr' : None, 'value' : JOB_STATE_STARTING }
                                        },
                                   'task' : {}
                                },
                    'step_init' : { 'job' : { 'user'       : { 'attr': "job_user", 'value' : None },
                                              'job_name'   : { 'attr' : "job_name", 'value' : None },
                                              'job_tag'    : { 'attr' : "subscriber_data.job_tag", 'value' : None },
                                              'node_count' : { 'attr' : "nnodes", 'value' : None },
                                              'job_size'   : { 'attr' : "total_tasks", 'value' : None },
                                              'app_id'     : { 'attr' : "step_id", 'value' : None },
                                              'job_uid'    : { 'attr' : "uid", 'value' : None },
                                              'job_gid'   : { 'attr' : "gid", 'value' : None },
                                              'task_count' : { 'attr' : "local_tasks", 'value' : None }
                                            },
                                    'task' : {}
                                 },
                    'task_init' : { 'job' : { 'job_id'      : { 'attr' : "job_id", "value" : None },
                                              'job_state'  : { 'attr' : None, 'value' : JOB_STATE_RUNNING }
                                            },
                                    'task' : { 'task_pid'    : { 'attr' : "task_pid", 'value' : None },
                                              'task_rank'  : { 'attr' : "task_global_id", 'value' : None },
                                              'task_exit_status'  : { 'attr' : "task_exit_status", 'value' : 0 }
                                            }
                                 },
                    'task_exit' : { 'job' : { 'job_state'  : { 'attr' : None, 'value' : JOB_STATE_COMPLETE }
                                            },
                                    'task' : { 'task_exit_status' : { 'attr' : "task_exit_status", 'value' : None },
                                             }
                                 },
                    'job_exit' : { 'job' : { 'job_end'     : { 'attr' : None, 'value' : None },
                                             'job_state'   : { 'attr' : None, 'value' : JOB_STATE_COMPLETE }
                                           }
                                 }
                }

def make_event_data(cont, event, job, task_idx = None):
    if task_idx is not None:
        task = job._tasks[cont.hostname][task_idx]
    else:
        task = None

    data = {    "schema": "slurm_job_data",
                "event" : None,
                "timestamp": int(time()),
                "context": "foo",
                "data": {}
           }

    if event == "job_init":
        data['event'] = "init"
        data['data'] = {
            "job_id" : job.job_id,
            "nodeid" : job._cont2node_id(cont),
            "uid" : job.uid,
            "gid" : job.gid,
            "ncpus" : job.ncpus, # Ignored by slurm_sampler2 and slurm_sampler
            "nnodes": 0, # Per my observation, slurm_notifier always sent 0
            "local_tasks": len(job._tasks[cont.hostname]),
            "total_tasks": job.total_tasks
        }
    elif event == "job_exit":
        data['event'] = "exit"
        data['data'] = {
            "job_id" : job.job_id,
            "nodeid" : job._cont2node_id(cont)
        }
    elif event == "step_init":
        data['event'] = "step_init"
        data['data'] = {
            "subscriber_data" : job.subscriber_data,
            "job_name" : job.job_name,
            "job_user" : job.user,
            "job_id" : job.job_id,
            "nodeid" : job._cont2node_id(cont),
            "step_id" : job.step_id,
            "alloc_mb" : job.alloc_mem, # Ignored by the plugins
            "ncpus" : job.ncpus, # Ignored by the plugins
            "nnodes" : job.nnodes,
            "local_tasks" : len(job._tasks[cont.hostname]), # Ignored by the plugins
            "total_tasks" : job.total_tasks,
        }
    elif event == "step_exit":
        data['event'] = "step_exit"
        data['data'] = {
            "job_id" : job.job_id,
            "nodeid" : job._cont2node_id(cont), # Ignored by the plugins
            "step_id" : job.step_id  # Ignored by the plugins
        }
    elif event == "task_init":
        data['event'] = "task_init_priv"
        data['data'] = {
            "job_id" : job.job_id,
            "step_id" : job.step_id, # Ignored by the plugin
            "task_id" : task.task_id, # Not used by slurm_sampler2
            "task_pid" : task.task_pid,
            "task_global_id" : task.task_global_id, # task_rank metric
            "nodeid" : job._cont2node_id(cont), # Ignored by the plugin
            "uid" : job.uid, # Ignored by the plugin
            "gid" : job.gid, # Ignored by the plugin
            "ncpus" : job.ncpus, # Ignored by the plugin
            "nnodes" : job.nnodes, # Ignored by the plugin
            "local_tasks" : len(job._tasks[cont.hostname]), # Ignored by the plugin
            "total_tasks" : job.total_tasks # Ignored by the plugin
        }
    elif event == "task_exit":
        data['event'] = "task_exit"
        data['data'] = {
            "job_id" : job.job_id,
            "step_id" : job.step_id, # Ignored by the plugins
            "task_id" : task.task_id, # Not used by slurm_sampler2
            "task_pid" : task.task_pid,
            "task_global_id" : task.task_global_id, # Ignored by the plugins
            "nodeid" : job._cont2node_id(cont), # Ignored by the plugins
            "task_exit_status" : task.task_exit_status
        }
    else:
        raise RuntimeError(f"Unrecognized {event}")
    return data

def publish_slurm_event(cont, data):
    fpath = f"/db/{cont.hostname}-{STREAM_DATA_FILE}-{data['data']['job_id']}-{data['event']}"
    cont.write_file(fpath, json.dumps(data))
    cmd = f"ldms_msg_publish -h localhost -x {LDMSD_XPRT} -p {LDMSD_PORT} " \
          f"-m slurm -t json -f {fpath}"
    rc, out = cont.exec_run(cmd)
    if rc:
        raise RuntimeError(f"Failed to run ldms_msg_publish. Error {rc}: {out}")

class Task(object):
    def __init__(self, node, job, task_rank, task_pid, task_exit_status = 0,
                 task_id = 0, ):
        self.job = job
        self.task_pid = task_pid
        self.task_global_id = task_rank
        self.task_exit_status = task_exit_status
        self._exited = False

        self.task_id = task_id

        self._node = node

        self.exp = None # The corresponding task entry in the task list in the set

    def __update_exp_metric(self, event):
        task_metrics = UPDATED_METRICS[event]['task']
        for k in task_metrics.keys():
            if task_metrics[k]['value'] is not None:
                self.exp[k] = task_metrics[k]['value']
            else:
                self.exp[k] = self.__dict__[task_metrics[k]['attr']]

    def task_init(self):
        task_metrics = UPDATED_METRICS['task_init']['task']
        self.exp = TASK_RECORD_DEFAULT_V.copy()
        self.exp['job_id'] = self.job.job_id
        self.__update_exp_metric('task_init')

    def task_exit(self):
        task_metrics = UPDATED_METRICS['task_exit']['task']
        self._exited = True
        self.__update_exp_metric('task_exit')

class Job(object):
    def _user2uid(self, user):
        if user == "root":
            return 0
        elif user == "user1":
            return 1000
        else:
            raise ValueError(f"Unexpected user {user}")

    def _cont2node_id(self, cont):
        if cont.hostname == "node-1":
            return 1
        elif cont.hostname == "node-2":
            return 2
        elif cont.hostname == "node-3":
            return 3
        elif cont.hostname == "node-4":
            return 4
        else:
            raise RuntimeError(f"Not recognized container {cont.hostname}")

    def __init__(self, job_id, uid = 0, gid = 0,
                 job_name = "job.sh", job_user = "", step_id = 0,
                 job_tag = "", ncpus = 0, alloc_mem = 0):
        self.job_id = job_id
        self.nnodes = 0
        self.total_tasks = 0
        self.uid  = uid
        self.gid = gid
        self.job_name = job_name
        self.subscriber_data = { 'job_tag' : job_tag }
        self.job_user = job_user
        self.step_id = step_id
        self.user = job_user

        self.ncpus = ncpus
        self.alloc_mem = alloc_mem

        self._nodes = []
        self._tasks = {}

        self.exp = {}  #  The corresponding job entry in the job_list in the set

    def add_task(self, node, task_rank, task_pid, task_exit_status = 0):
        task = Task(node, self, task_rank, task_pid, task_exit_status)
        if node.hostname not in self._tasks.keys():
            self._tasks[node.hostname] = []
            self.nnodes += 1
            self._nodes.append(node)
        self._tasks[node.hostname].append(task)
        self.total_tasks += 1

    def __update_exp_metrics(self, event, node):
        job_metrics = UPDATED_METRICS[event]['job']
        exp = self.exp[node.hostname]
        for k in job_metrics.keys():
            if job_metrics[k]['value'] is not None:
                exp[k] = job_metrics[k]['value']
            elif job_metrics[k]['attr'] is not None:
                attr = job_metrics[k]['attr']
                if attr == "local_tasks":
                    exp[k] = len([t for t in self._tasks[node.hostname] if not t._exited])
                elif "subscriber_data" in attr:
                    exp[k] = self.__dict__["subscriber_data"][attr.split('.')[1]]
                else:
                    exp[k] = self.__dict__[attr]
            else:
                    exp[k] = None

    def job_init(self, publish = True):
        if publish:
            for node in self._nodes:
                data = make_event_data(node, "job_init", self)
                publish_slurm_event(node, data)

        # Prep expecting results
        for node in self._nodes:
            self.exp[node.hostname] = JOB_RECORD_DEFAULT_V.copy()
            self.__update_exp_metrics("job_init", node)

    def step_init(self, publish = True):
        if publish:
            for node in self._nodes:
                data = make_event_data(node, "step_init", self)
                publish_slurm_event(node, data)

        # Prep expecting results
        for node in self._nodes:
            self.__update_exp_metrics("step_init", node)

    def task_init(self, publish = True):
        for node in self._nodes:
            for i in range(len(self._tasks[node.hostname])):
                self._tasks[node.hostname][i].task_init()

        if publish:
            for node in self._nodes:
                for i in range(len(self._tasks[node.hostname])):
                    data = make_event_data(node, "task_init", self, i)
                    publish_slurm_event(node, data)

        for node in self._nodes:
            self.__update_exp_metrics("task_init", node)

    def task_exit(self, publish = True):
        for node in self._nodes:
            for i in range(len(self._tasks[node.hostname])):
                self._tasks[node.hostname][i].task_exit()

        if publish:
            for node in self._nodes:
                for i in range(len(self._tasks[node.hostname])):
                    data = make_event_data(node, "task_exit", self, i)
                    publish_slurm_event(node, data)

        for node in self._nodes:
            self.__update_exp_metrics("task_exit", node)

    def job_exit(self, publish = True):
        if publish:
            for node in self._nodes:
                data = make_event_data(node, "job_exit", self)
                publish_slurm_event(node, data)

        for node in self._nodes:
            self.__update_exp_metrics("job_exit", node)

    op = {'job_init': job_init,
          'step_init': step_init,
          'task_init' : task_init,
          'task_exit' : task_exit,
          'job_exit' : job_exit}

def ldms_ls(cont, host = "localhost", xprt = LDMSD_XPRT, port = LDMSD_PORT, l = True):
    rc, out = cont.exec_run(f"/tada-src/python/ldms_ls.py -x {LDMSD_XPRT} " \
                                        f"-p {LDMSD_PORT} -h localhost -l")
    if rc:
        raise RuntimeError(f"ldms_ls.py error {rc}, out: {out}")
    obj = json.loads(out)
    return obj

def ldms_ls_all():
    results = {}
    for node in nodes:
        results[node.hostname] = ldms_ls(node)
    return results

def get_ldmsd_spec(cont):
    return [d for d in cont.spec['daemons'] if d['type'] == "ldmsd"][0]

def get_sampler_plugin_spec(ldmsd_spec, plugin):
    return [p for p in ldmsd_spec['samplers'] if p['plugin'] == plugin][0]

def get_spec_component_id(plugin_spec):
    config = plugin_spec['config']
    for av in config:
        a, v = av.split("=")
        if a == "component_id":
            return int(v)
    raise KeyError(f"Cannot find component_id in {config}")

def get_spec_producer(plugin_spec):
    config = plugin_spec['config']
    for av in config:
        a, v = av.split("=")
        if a == "producer":
            return v
    raise KeyError(f"Cannot find producer in {config}")

def verify_set(node, set, jobs):
    global cluster
    job_list = set['data']['job_list']
    task_list = set['data']['task_list']
    comp_id = int(set['data']['component_id'])
    producer = set['producer_name']

    cont = cluster.get_container(node)
    slurm2_spec = get_sampler_plugin_spec(get_ldmsd_spec(cont), "slurm_sampler2")
    exp_job_list = [job.exp[node] for job in jobs]
    exp_task_list = [task.exp for job in jobs for task in job._tasks[node] if task.exp is not None]
    exp_comp_id = get_spec_component_id(slurm2_spec)
    exp_producer = get_spec_producer(slurm2_spec)

    if comp_id != exp_comp_id:
        return (False, f"The component_id {comp_id} is not the expected value {exp_comp_id}.")
    if producer != exp_producer:
        return (False, f"The producer '{producer}' is not the expected value '{exp_producer}'.")
    # Ignore the job's start and end time
    for j in job_list:
        j['job_start'] = None
        j['job_end'] = None
    # Assume both job_list and exp_job_list are ordered by job_id
    if job_list != exp_job_list:
        if len(job_list) != len(exp_job_list):
            return (False, f"The number of jobs is not as expected. {len(job_list)} != {len(exp_job_list)}.")
        for a, b in zip(job_list, exp_job_list):
            if a != b:
                return (False, f"The job_list is not as expected. {a} != {b}")
    if task_list != exp_task_list:
        if len(task_list) != len(exp_task_list):
            return (False, f"The number of tasks is not as expected. {len(task_list)} != {len(exp_task_list)}.")
        for a, b in zip(task_list, exp_task_list):
            if a != b:
                return (False, f"The task_list is not as expected. {a} != {b}")
    return (True, None)

def verify_results(results, jobs):
    if results.keys() != jobs.keys():
        raise RuntimeError(f"results.keys() != jobs.keys()")
    for node in results.keys():
        set = results[node][f"{node}/slurm_sampler2"]
        job_list = jobs[node]
        passed, reason = verify_set(node, set, job_list)
        if not passed:
            return (False, f"[{node}]: {reason}")
    return (True, None)

job_id = assertion_id_get()
def get_job_id():
    return next(job_id) + 1000

def get_task_pid(job_id, rank):
    return job_id * 100 + rank

def test_all_step(job, nodes_job, assertion):
    for e in STEPD_EVENT:
        job.op[e](job)
        sleep(1)
        results = ldms_ls_all()
        passed, reason = verify_results(results, node_jobs)
        test.assert_test(assertion[e], passed,
                         "The metric values are as expected on all nodes." if passed else reason)

id = assertion_id_get()
SLURM_NOTIFIER = next(id)
no = next(id)
DELETE_COMPLETE_JOBS = dict(zip(STEPD_EVENT, [no + x/10 for x in range(1, len(STEPD_EVENT)+1)]))
no = next(id)
EXPAND_HEAP = dict(zip(STEPD_EVENT, [no + x/10 for x in range(1, len(STEPD_EVENT)+1)]))
no = next(id)
MULTI_TENANTS = dict(zip(STEPD_EVENT, [no + x/10 for x in range(1, len(STEPD_EVENT)+1)]))

#### Test Definition ####
test = TADA.Test(test_suite = "LDMSD",
                 test_type = "FVT",
                 test_name = "slurm_sampler2_test",
                 test_desc = "Test the slurm_sampler2 plugin",
                 test_user = args.user,
                 commit_id = args.commit_id,
                 tada_addr = args.tada_addr)
test.add_assertion(SLURM_NOTIFIER,
                   "Processing the stream data from slurm_notifier")
for e in STEPD_EVENT:
    test.add_assertion(DELETE_COMPLETE_JOBS[e],
                   f"Deleting completed jobs -- {e}")
    test.add_assertion(EXPAND_HEAP[e],
                   f"Expanding the set heap -- {e}")
    test.add_assertion(MULTI_TENANTS[e],
                   f"Multi-tenant -- {e}")

@atexit.register
def at_exit():
    global EXIT_RC
    test.finish()
    if cluster is not None:
        cluster.remove()
    os._exit(EXIT_RC)

#### Start ####
cluster = None
test.start()

log.info("-- Get or create the cluster --")
cluster = LDMSDCluster.get(spec["name"], create = True, spec = spec)
cleanup_db(cluster)

log.info("-- Add users --")
for u in USERS.keys():
    cluster.all_exec_run(f"adduser --uid {USERS[u]} {u}")

log.info("-- Preparing job script & programs --")

cont = cluster.get_container("headnode")

code = """\
#include <stdio.h>
#include <unistd.h>

int main(int argc, char **argv)
{
    int time = 5; /* 5 seconds */
    int interval = 5; /* Report every 5 seconds */
    int i = interval;

    if (argc > 1)
        time = atoi(argv[1]);

    int x = time / interval;
    int remainder = time - interval * x;

    printf("Run for %d seconds\\n", time);
    while (i <= time) {
        sleep(interval);
        i += interval;
    }
    sleep(remainder);
    printf("done\\n");
    return 0;
}
"""
cont.write_file("/db/prog.c", code)

rc, out = cont.exec_run("gcc -o /db/prog /db/prog.c")
assert(rc == 0)

code = """\
#!/bin/bash

echo start: $(date +%s)

srun /db/prog $1

echo finish: $(date +%s)
"""
cont.write_file("/db/job.sh", code)
cont.exec_run("sync")

log.info("-- Start daemons --")
cluster.start_daemons()
cluster.make_known_hosts()
sleep(1) # Wait for the daemons to start

nodes = [cluster.get_container(f"node-{c}") for c in range(1, 5)]

# -------------------------------------------
node_jobs = dict.fromkeys([node.hostname for node in nodes])

# ------------------------------------------------------------------------------
def test_slurm_notifier(node_name, set, exp_job_list, exp_task_list):
    job_list = set['data']['job_list']
    for job in job_list:
        job['job_start'] = None
        job['job_end'] = None

    if len(job_list) != len(exp_job_list):
        return (False, f"[{node_name}]: The job_list's length ({len(job_list)}) is not as expected ({len(exp_job_list)}).")
    if job_list != exp_job_list:

        for a, b in zip(job_list, exp_job_list):
            if a != b:
                return (False, f"[{node_name}]: The job_list is not as expected. {a} != {b}")

    task_list = set['data']['task_list']
    if len(task_list) != len(exp_task_list):
        return (False, f"[{node_name}]: The task_list's length ({len(task_list)}) is not as expected ({len(exp_task_lsit)}).")

    for a, b in zip(task_list, exp_task_list):
        if a['job_id'] != b['job_id'] or a['task_exit_status'] != b['task_exit_status']:
            return (False, f"[{node_name}]: The task list is not as expected. {a} != {b}")
    return (True, None)

job_0_id = cluster.sbatch("/db/job.sh 1",
                        "--nodelist=node-[1-4]",
                       f"--ntasks-per-node={CPU_PER_NODE}")

exp_job_0 = JOB_RECORD_DEFAULT_V.copy()
exp_job_0['job_id'] = job_0_id
exp_job_0['user'] = "root"
exp_job_0['job_uid'] = 0
exp_job_0['job_name'] = "job.sh"
exp_job_0['job_state'] = JOB_STATE_COMPLETE
exp_job_0['task_count'] = CPU_PER_NODE
exp_job_0['node_count'] = 4
exp_job_0['job_size'] = exp_job_0['task_count'] * exp_job_0['node_count']
exp_job_0['job_start'] = exp_job_0['job_end'] = None
exp_job_0_task = TASK_RECORD_DEFAULT_V.copy()
exp_job_0_task['job_id'] = exp_job_0['job_id']
exp_job_0_task['task_pid'] = exp_job_0_task['task_rank'] = None
exp_job_0_task['task_rank'] = None

sleep(5) # wait for job to complete
results = ldms_ls_all()
for node in nodes:
    set = results[node.hostname][f"{node.hostname}/slurm_sampler2"]

    exp_job_list = [exp_job_0]
    exp_task_list = [exp_job_0_task, exp_job_0_task]
    passed, reason = test_slurm_notifier(node.hostname, set,
                                         exp_job_list, exp_task_list)
    if not passed:
        break
test.assert_test(SLURM_NOTIFIER, passed,
                 "The metric values are as expected on all nodes." if passed else reason)

job_0 = Job(job_id = job_0_id, job_user = "root")
for node in nodes:
    set = results[node.hostname][f"{node.hostname}/slurm_sampler2"]
    job_0.add_task(node, task_rank = set['data']['task_list'][0]['task_rank'],
                   task_pid = set['data']['task_list'][0]['task_pid'],
                   task_exit_status = 0)
    job_0.add_task(node, task_rank = set['data']['task_list'][1]['task_rank'],
                   task_pid = set['data']['task_list'][1]['task_pid'],
                   task_exit_status = 0)
    node_jobs[node.hostname] = [job_0]
    job_0.exp[node.hostname] = exp_job_0.copy()
for e in STEPD_EVENT:
    job_0.op[e](job_0, publish = False)

# Job 1 -- to test the delete job time
sleep(DELETE_JOB_TIME)
del node_jobs['node-3'][0]
del node_jobs['node-4'][0]

job_1 = Job(job_id = get_job_id())
task_rank = assertion_id_get()
for node in nodes:
    rank = next(task_rank)
    job_1.add_task(node, rank, get_task_pid(job_1.job_id, rank))
    node_jobs[node.hostname].append(job_1)

test_all_step(job_1, node_jobs, DELETE_COMPLETE_JOBS)

# ------------------------------------------------------------------------------
# job 2 -- to test expanding the set heap
sleep(DELETE_JOB_TIME)
del node_jobs['node-3'][0]
del node_jobs['node-4'][0]

job_2 = Job(job_id = get_job_id(), uid = 1000, gid = 1000, job_user = "foo", job_tag = "bar", ncpus = 2)
task_rank = assertion_id_get()
num_task_per_node = dict(zip([node.hostname for node in nodes], range(1, 1 + len(nodes))))
for node in nodes:
    for i in range(0, num_task_per_node[node.hostname]):
        rank = next(task_rank)
        job_2.add_task(node, rank, get_task_pid(job_2.job_id, rank))
    node_jobs[node.hostname].append(job_2)
test_all_step(job_2, node_jobs, EXPAND_HEAP)

# ------------------------------------------------------------------------------
# job_3 and job_4 --- test multitenant

sleep(DELETE_JOB_TIME)
del node_jobs['node-3'][0]
del node_jobs['node-4'][0]

job_3 = Job(job_id = get_job_id())
task_rank = assertion_id_get()
for node in nodes:
    rank = next(task_rank)
    job_3.add_task(node, rank, get_task_pid(job_3.job_id, rank))
    node_jobs[node.hostname].append(job_3)

job_4 = Job(job_id = get_job_id())
task_rank = assertion_id_get()
for node in nodes:
    for i in range(2):
        rank = next(task_rank)
        job_4.add_task(node, rank, get_task_pid(job_4.job_id, rank))
    node_jobs[node.hostname].append(job_4)

job_3.job_init()
job_3.step_init()
job_3.task_init()
job_4.job_init()

results = ldms_ls_all()
passed, reason = verify_results(results, node_jobs)
test.assert_test(MULTI_TENANTS["job_init"], passed,
                 "The metric values are as expected on all nodes." if passed else reason)

job_4.step_init()
results = ldms_ls_all()
passed, reason = verify_results(results, node_jobs)
test.assert_test(MULTI_TENANTS["step_init"], passed,
                 "The metric values are as expected on all nodes." if passed else reason)

job_4.task_init()
results = ldms_ls_all()
passed, reason = verify_results(results, node_jobs)
test.assert_test(MULTI_TENANTS["task_init"], passed,
                 "The metric values are as expected on all nodes." if passed else reason)

job_4.task_exit()
results = ldms_ls_all()
passed, reason = verify_results(results, node_jobs)
test.assert_test(MULTI_TENANTS["task_exit"], passed,
                 "The metric values are as expected on all nodes." if passed else reason)

job_4.job_exit()
results = ldms_ls_all()
passed, reason = verify_results(results, node_jobs)
test.assert_test(MULTI_TENANTS["job_exit"], passed,
                 "The metric values are as expected on all nodes." if passed else reason)
EXIT_RC = 0
