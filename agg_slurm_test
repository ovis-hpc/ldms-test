#!/usr/bin/env python3

# Setup:
#   - 2 samplers, each has the following samplers: meminfo, slurm_sampler,
#     syspapi_sampler, papi_sampler
#   - agg11 and agg12 as L1 aggregators, agg11 connects to sampler1 and agg12
#     connects to sampler2
#   - agg2 as L2 aggregator with:
#     - store_sos for meminfo data
#     - store_sos for syspapi_sampler data
#     - store_slurm for slurm_sampler data
#     - store_papi for papi_sampler data
#
# Scenario:
#   - all ldmsds start
#   - wait a while so that syspapi data can populate the store
#   - submit job1
#   - submit job2
#   - wait a while, expecting syspapi to pause and data from papi_sampler shall
#     populate store_papi
#   - cancel job1 and job2
#   - wait for a while, expecting syspapi to resume and its data continue
#     populating the store.
#   - terminate all ldmsds
#   - verify the store data

import os
import re
import pwd
import sys
import json
import time
import argparse
import TADA
import logging
import atexit

from distutils.spawn import find_executable
from LDMS_Test import LDMSDCluster, LDMSDContainer, process_args, \
                      add_common_args, ldmsd_version

if __name__ != "__main__":
    raise RuntimeError("This should not be impoarted as a module.")

class Debug(object):
    pass
D = Debug()

logging.basicConfig(format = "%(asctime)s %(name)s %(levelname)s %(message)s",
                    level = logging.INFO)

log = logging.getLogger(__name__)

exec(open(os.getenv("PYTHONSTARTUP", "/dev/null")).read())

def jprint(obj):
    """Pretty print JSON object"""
    print(json.dumps(obj, indent=2))

#### default values #### ---------------------------------------------
sbin_ldmsd = find_executable("ldmsd")
if sbin_ldmsd:
    default_prefix, a, b = sbin_ldmsd.rsplit('/', 2)
else:
    default_prefix = "/opt/ovis"

#### argument parsing #### -------------------------------------------
ap = argparse.ArgumentParser(description =
                         "Run test scenario of 2 samplers " \
                         "-> 2 x agg-1 -> agg-2 " \
                         "with slurm job ID verification." )
add_common_args(ap)
ap.add_argument("--slurm-notifier", type = str,
                default = "__find_from_prefix__",
                help = "The path (in container) to slurm_notifier library." )
ap.add_argument("--num-compute", type = int,
                default = 2,
                help = "Number of compute nodes.")
args = ap.parse_args()
process_args(args)


#### config variables #### ------------------------------
USER = args.user
PREFIX = args.prefix
COMMIT_ID = args.commit_id
SRC = args.src
CLUSTERNAME = args.clustername
DB = args.data_root
NUM_COMPUTE = args.num_compute
LDMSD_VERSION = ldmsd_version(PREFIX)
DEBUG = args.debug

STORE_ROOT = "/store" # path inside container (agg-2)

SLURM_NOTIFIER = args.slurm_notifier
if SLURM_NOTIFIER == "__find_from_prefix__":
    paths = map(lambda x: "{}/{}/ovis-ldms/libslurm_notifier.so"\
                          .format(PREFIX, x),
                ["lib", "lib64"])
    for p in paths:
        if os.path.exists(p):
            SLURM_NOTIFIER = p.replace(PREFIX, '/opt/ovis', 1)
            break
    else:
        raise RuntimeError("libslurm_notifier.so not found")


#### spec #### -------------------------------------------------------
common_plugin_config = [
        "component_id=%component_id%",
        "instance=%hostname%/%plugin%",
        "producer=%hostname%",
        "job_set=%hostname%/slurm_sampler",
    ]
spec = {
    "name" : CLUSTERNAME,
    "description" : "{}'s test_agg_slurm cluster".format(USER),
    "type" : "NA",
    "templates" : { # generic template can apply to any object by "!extends"
        "compute-node" : {
            "daemons" : [
                {
                    "name" : "sshd",
                    "type" : "sshd",
                },
                {
                    "name" : "munged",
                    "type" : "munged",
                },
                {
                    "name" : "slurmd",
                    "!extends" : "slurmd",
                },
                {
                    "name" : "sampler-daemon",
                    "!extends" : "ldmsd-sampler",
                },
            ],
        },
        "slurmd" : {
            "type" : "slurmd",
            "plugstack" : [
                {
                    "required" : True,
                    "path" : SLURM_NOTIFIER,
                    "args" : [
                        "auth=none",
                        "port=10000",
                        "client=sock:localhost:10000:none",
                    ],
                },
            ],
        },
        "sampler_plugin" : {
            "interval" : 1000000,
            "offset" : 0,
            "config" : common_plugin_config,
            "start" : True,
        },
        "ldmsd-base" : {
            "type" : "ldmsd",
            "listen_port" : 10000,
            "listen_xprt" : "sock",
            "listen_auth" : "none",
        },
        "ldmsd-sampler" : {
            "!extends" : "ldmsd-base",
            "samplers" : [
                {
                    "plugin" : "slurm_sampler",
                    "!extends" : "sampler_plugin",
                    "start" : False, # override
                },
                {
                    "plugin" : "meminfo",
                    "exclusive_thread": 1,
                    "!extends" : "sampler_plugin",
                },
                {
                    "plugin" : "syspapi_sampler",
                    "!extends" : "sampler_plugin",
                    "config" : common_plugin_config + [
                        "cfg_file=/db/syspapi.json",
                        "cumulative=0",
                        "auto_pause=1",
                    ],
                },
                {
                    "plugin" : "papi_sampler",
                    "!extends" : "sampler_plugin",
                    "config" : common_plugin_config + [
                        "job_expiry=5",
                    ]
                },
            ],
        },
        "prdcr" : {
            "host" : "%name%",
            "port" : 10000,
            "xprt" : "sock",
            "type" : "active",
            "interval" : 1000000,
        },
        "ldmsd-aggregator" : {
            "!extends" : "ldmsd-base",
            "config" : [ # additional config applied after prdcrs
                "prdcr_start_regex regex=.*",
                "updtr_add name=all interval=1000000 offset=%offset%",
                "updtr_prdcr_add name=all regex=.*",
                "updtr_start name=all"
            ],
        },
    }, # templates
    "nodes" : [
        {
            "hostname" : "node-{}".format(i),
            "component_id" : 10000 + i,
            "!extends" : "compute-node",
        } for i in range(1, NUM_COMPUTE+1)
    ] + [
        {
            "hostname" : "agg-1{}".format(j),
            "!extends" : "compute-node",
            "daemons" : [
                {
                    "name" : "sshd",
                    "type" : "sshd",
                },
                {
                    "name" : "agg",
                    "!extends" : "ldmsd-aggregator",
                    "offset" : 250000,
                    "prdcrs" : [ # these producers will turn into `prdcr_add`
                        {
                            "name" : "node-{}".format(i*2+j),
                            "!extends" : "prdcr",
                        } for i in range(0, int(NUM_COMPUTE / 2))
                    ],
                },
            ]
        } for j in [1, 2]
    ] + [
        {
            "hostname" : "agg-2",
            "daemons" : [
                {
                    "name" : "sshd",
                    "type" : "sshd",
                },
                {
                    "name" : "aggregator",
                    "!extends" : "ldmsd-aggregator",
                    "offset" : 500000,
                    "prdcrs" : [ # these producers will turn into `prdcr_add`
                        {
                            "name" : "agg-1{}".format(i),
                            "!extends" : "prdcr",
                        } for i in [1,2]
                    ],
                    # additional config applied after prdcrs
                    "config" : [
                        # -- for v5 --
                        "load name=papi plugin=store_papi",
                        "config name=papi path={}/papi/papi".format(STORE_ROOT),
                        "strgp_add name=papi_store" \
                                 " container=papi schema=jobpapi",
                        "strgp_prdcr_add name=papi_store regex=.*",
                        "strgp_start name=papi_store",

                        "load name=syspapi plugin=store_sos",
                        "config name=syspapi path={}/sos/syspapi".format(STORE_ROOT),
                        "strgp_add name=syspapi_store" \
                                 " container=syspapi schema=syspapi",
                        "strgp_prdcr_add name=syspapi_store regex=.*",
                        "strgp_start name=syspapi_store",

                        "load name=meminfo plugin=store_sos",
                        "config name=meminfo path={}/sos/meminfo".format(STORE_ROOT),
                        "strgp_add name=meminfo_store" \
                                 " container=meminfo schema=meminfo",
                        "strgp_prdcr_add name=meminfo_store regex=.*",
                        "strgp_start name=meminfo_store",

                        "load name=slurm plugin=store_slurm",
                        "config name=slurm path={}/slurm/slurm verbosity=1".format(STORE_ROOT),
                        "strgp_add name=slurm_store" \
                                 " container=slurm schema=mt-slurm",
                        "strgp_prdcr_add name=slurm_store regex=.*",
                        "strgp_start name=slurm_store",

                        "prdcr_start_regex regex=.*",
                        "updtr_add name=all interval=1000000 offset=%offset%",
                        "updtr_prdcr_add name=all regex=.*",
                        "updtr_start name=all",
                    ] if LDMSD_VERSION >= (4, 100, 0) else [
                        # -- for v4 --
                        "load name=store_sos",
                        "config name=store_sos path={}/sos".format(STORE_ROOT),
                        "load name=store_papi",
                        "config name=store_papi path={}/papi".format(STORE_ROOT),
                        "load name=store_slurm",
                        "config name=store_slurm path={}/slurm verbosity=1".format(STORE_ROOT),

                        "strgp_add name=papi_store plugin=store_papi" \
                                 " container=papi schema=jobpapi",
                        "strgp_prdcr_add name=papi_store regex=.*",
                        "strgp_start name=papi_store",

                        "strgp_add name=syspapi_store plugin=store_sos" \
                                 " container=syspapi schema=syspapi",
                        "strgp_prdcr_add name=syspapi_store regex=.*",
                        "strgp_start name=syspapi_store",

                        "strgp_add name=meminfo_store plugin=store_sos" \
                                 " container=meminfo schema=meminfo",
                        "strgp_prdcr_add name=meminfo_store regex=.*",
                        "strgp_start name=meminfo_store",

                        "strgp_add name=slurm_store plugin=store_slurm" \
                                 " container=slurm schema=mt-slurm",
                        "strgp_prdcr_add name=slurm_store regex=.*",
                        "strgp_start name=slurm_store",

                        "prdcr_start_regex regex=.*",
                        "updtr_add name=all interval=1000000 offset=%offset%",
                        "updtr_prdcr_add name=all regex=.*",
                        "updtr_start name=all",
                    ],
                },
            ],
        },
        {
            "hostname" : "headnode",
            "daemons" : [
                {
                    "name" : "sshd",
                    "type" : "sshd",
                },
                {
                    "name" : "slurmctld",
                    "type" : "slurmctld",
                },
            ]
        },
    ], # nodes

    #"image": "ovis-centos-build:slurm",
    "cpu_per_node": 2,
    "slurm_loglevel" : "debug2",
    "oversubscribe" : "FORCE",
    "cap_add": [ "SYS_PTRACE", "SYS_ADMIN" ],
    "image": args.image,
    "ovis_prefix": PREFIX,
    "env" : { "FOO": "BAR" },
    "mounts": [
        "{}:/db:rw".format(DB),
    ] + args.mount +
    ( ["{0}:{0}:ro".format(SRC)] if SRC else [] ),
}

#### test definition ####

test = TADA.Test(test_suite = "LDMSD",
                 test_type = "FVT",
                 test_name = "agg_slurm_test",
                 test_desc = "LDMSD 2-level agg with slurm",
                 test_user = args.user,
                 commit_id = COMMIT_ID,
                 tada_addr = args.tada_addr)
test.add_assertion(1, "ldms_ls agg-2")
test.add_assertion(2, "slurm data verification")
test.add_assertion(3, "meminfo data verification")
test.add_assertion(4, "(SYS/JOB) PAPI data verification")

#### Clean up db ####
def cleanup_db(cluster):
    cont = cluster.get_container("headnode")
    LST = [ "job.sh", "jobpapi.json", "prog", "prog.c", "slurm*.out", "syspapi.json" ]
    LST = [ "/db/{}".format(x) for x in LST ]
    LST += [ "{}/{}".format(STORE_ROOT, x) for x in ["sos", "slurm", "papi"] ]
    cont = cluster.get_container("agg-2")
    cont.exec_run("rm -rf {}".format(" ".join(LST)))

#### Start! ####
cluster = None
test.start()

@atexit.register
def at_exit():
    rc = test.finish()
    if cluster is not None:
        cluster.remove()
    os._exit(rc)

log.info("-- Get or create the cluster --")
cluster = LDMSDCluster.get(spec["name"], create = True, spec = spec)
cleanup_db(cluster)

cont = cluster.get_container("headnode")
agg2 = cluster.get_container("agg-2")

agg2.exec_run("mkdir -p {}/sos".format(STORE_ROOT))
agg2.exec_run("mkdir -p {}/papi".format(STORE_ROOT))
agg2.exec_run("mkdir -p {}/slurm".format(STORE_ROOT))

log.info("-- Preparing syspapi JSON file --")
papi_json = {
            "schema" : "syspapi",
            "events" : [
                "PAPI_L1_DCM",
                "PAPI_TOT_INS",
                "PAPI_TOT_CYC",
            ]
        }
cont.write_file("/db/syspapi.json", json.dumps(papi_json, indent=2))

log.info("-- Preparing jobpapi JSON file --")
papi_json = {
            "schema" : "jobpapi",
            "events" : [
                "PAPI_L1_DCM",
                "PAPI_TOT_INS",
                "PAPI_TOT_CYC",
            ]
        }
cont.write_file("/db/jobpapi.json", json.dumps(papi_json, indent=2))

log.info("-- Preparing job script & programs --")

code = """\
#include <stdio.h>
#include <unistd.h>

int main(int argc, char **argv)
{
    int n = 4096, N = 15;
    int sum, i, j;
    for (j = 0; j < N; j++) {
        sum = 0;
        for (i = 0; i < n; i++) {
            sum += i;
        }
        printf("sum: %d\\n", sum);
        sleep(1);
    }
    return 0;
}
"""
cont.write_file("/db/prog.c", code)

rc, out = cont.exec_run("gcc -o /db/prog /db/prog.c")
assert(rc == 0)

code = """\
#!/bin/bash

#SBATCH -n 2
#SBATCH -D /db

export SUBSCRIBER_DATA='{ "papi_sampler": { "file": "/db/jobpapi.json" } }'
srun /db/prog
"""
cont.write_file("/db/job.sh", code)

cont.exec_run("sync")
cluster.files_exist(["/db/syspapi.json", "/db/jobpapi.json", "/db/prog.c"])

log.info("-- Start daemons --")
cluster.start_daemons()
cluster.make_known_hosts()

log.info("... wait a bit to make sure ldmsd's are up")
time.sleep(5)

cont = cluster.get_container("headnode")

log.info("-- ldms_ls to agg-2 --")
rc, out = cont.ldms_ls("-x sock -p 10000 -h agg-2")

expect = set([ "node-{}/{}".format(i, s) \
                    for i in range(1, NUM_COMPUTE+1) \
                    for s in ["slurm_sampler", "meminfo",
                              "syspapi_sampler"] ])
result = set(out.splitlines())

test.assert_test(1, expect == result, "dir result verified")

# Now, test slurm_sampler for new job
for job in cluster.squeue():
    log.info("Cancelling job {}".format(job["JOBID"]))
    cluster.scancel(job["JOBID"])

log.info("-- Give syspapi some time to work before submitting job --")
time.sleep(5)

log.info("-- Submitting jobs --")
job_one = cluster.sbatch("/db/job.sh")
log.info("job_one: {}".format(job_one))
job_two = cluster.sbatch("/db/job.sh")
log.info("job_two: {}".format(job_two))
time.sleep(10)
log.info("-- Cancelling jobs --")
log.info("job_one: {}".format(job_one))
cluster.scancel(job_one)
log.info("job_two: {}".format(job_two))
cluster.scancel(job_two)
time.sleep(60)

agg2.kill_ldmsd()
time.sleep(5)
for C in cluster.containers:
    C.kill_ldmsd()

time.sleep(5)

def sos_query(dcont, scont, schema, idx):
    rc, out = dcont.exec_run("sos_cmd -C {} -q -S {} -X {} -f json"\
                             .format(scont, schema, idx))
    if rc != 0:
        raise RuntimeError("sos_cmd error {}, out: {}".format(rc, out))
    data = json.loads(out)
    data = data['data']
    for d in data:
        d['schema'] = schema
    return data

def check_timestamp(data, interval=1):
    # separate data by components
    comps = {}
    for obj in data:
        comp_id = obj["component_id"]
        comp_data = comps.setdefault(comp_id, list())
        comp_data.append(obj)
    count = 0
    D.comps = comps
    jumps = []
    for comp_data in comps.values():
        comp_data.sort(key = lambda x: float(x['timestamp']))
        itr = iter(comp_data)
        prev = next(itr)
        for x in itr:
            d = float(x['timestamp']) - float(prev['timestamp'])
            if d > 1.5 * interval:
                count += 1
                jumps.append((prev, x))
            prev = x
    return count, jumps

def check_papi(data):
    # check papi/syspapi data
    data.sort(key = lambda x: float(x['timestamp']))
    comps = {}
    for obj in data:
        comp_id = obj["component_id"]
        comp_data = comps.setdefault(comp_id, list())
        comp_data.append(obj)
    is_syspapi = lambda x: x['schema'] == 'syspapi'
    is_not_syspapi = lambda x: x['schema'] != 'syspapi'
    chk = [is_syspapi, is_not_syspapi]
    _ret = {}
    for comp_id, comp_data in comps.items():
        switches = []
        chk_idx = 0
        check = chk[chk_idx]
        for d in comp_data:
            if check(d):
                continue
            # switch the checking function
            switches.append(d)
            chk_idx = int(not chk_idx)
            check = chk[chk_idx]
        _ret[comp_id] = switches
    return _ret

slurm = sos_query(agg2, "/{}/slurm/slurm".format(STORE_ROOT), "mt-slurm", "time_job")
papi_tot_cyc = sos_query(agg2, "/{}/papi/papi".format(STORE_ROOT), "PAPI_TOT_CYC", "time_job")
papi_sr_ins = sos_query(agg2, "/{}/papi/papi".format(STORE_ROOT), "PAPI_TOT_INS", "time_job")
papi_ld_ins = sos_query(agg2, "/{}/papi/papi".format(STORE_ROOT), "PAPI_L1_DCM", "time_job")
syspapi = sos_query(agg2, "/{}/sos/syspapi".format(STORE_ROOT), "syspapi", "time_job_comp")
meminfo = sos_query(agg2, "/{}/sos/meminfo".format(STORE_ROOT), "meminfo", "time_job_comp")

# Slurm data check
#test.add_assertion(2, "slurm data verification")
slurm_expect = set([(j,r, "job.sh", "root") for j in [job_one, job_two] \
                          for r in [0,1]])
s = set([ (int(o['job_id']), int(o['task_rank']), o['job_name'], o['job_user'] ) for o in slurm ])
test.assert_test(2, s == slurm_expect, "get expected data from store")

# meminfo data check
#test.add_assertion(3, "meminfo data verification")
# allow some *jump* on old hardware (like cygnus) ... meminfo sampling cycle was
# affected by switching to/fromg syspapi <-> papi_sampler
missing_counts, jumps = check_timestamp(meminfo)
test.assert_test(3, len(meminfo) > 5 and missing_counts <= 2, "checked")

# papi data check
#test.add_assertion(4, "(SYS/JOB) PAPI data verification")
papi = syspapi + papi_ld_ins + papi_tot_cyc + papi_sr_ins
comp_switches = check_papi(papi)
for comp, switches in comp_switches.items():
    # For each component, we expect the storing data to be syspapi first, then
    # switch to papi (b/c job started), then switch back to syspapi (b/c job
    # terminated). So, we expect 2 switches.
    if len(switches) != 2:
        test.assert_test(4, False, "Wrong papi_sampler/syspapi_sampler "
                                   "switching: {}".format(switches))
        break
else: # loop does not break
    if not papi_tot_cyc:
        test.assert_test(4, False, "No PAPI_TOT_CYC data")
    elif not papi_sr_ins:
        test.assert_test(4, False, "No PAPI_TOT_INS data")
    elif not papi_ld_ins:
        test.assert_test(4, False, "No PAPI_L1_DCM data")
    elif not syspapi:
        test.assert_test(4, False, "No SYSPAPI data")
    else:
        test.assert_test(4, True, "No data missing")
# see `at_exit()` function
