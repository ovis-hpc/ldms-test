#!/usr/bin/env python3

# REMARK This test uses `ldms-kafka` image to run the test
#
# Test `flex` (with `as_is` and `static`)  decomposition with  `store_sos`,
# `store_csv`, and `store_kafka`.
#
# There are 4 sets in this test (2 sets from 2 samplers):
#   - samp-1/test_sampler
#     "8D2B8BD27CD9C888010D9B24F52F8E80E4AA2EAB2AD8690D7845902F2B2CDA12"
#   - samp-1/record_sampler
#     "E1F021F9273566F3675C1378CD29C6A3769AEE4178C198704E4D6A9130065F2B"
#   - samp-2/test_sampler
#     "95772B66C5BCDAFE2C40E3B60063258DB0E13F4E235F0F26F97F42FFBB986ED4"
#   - samp-2/record_sampler
#     "E1F021F9273566F3675C1378CD29C6A3769AEE4178C198704E4D6A9130065F2B"
#
# `samp-1/test_sampler` and `samp-2/test_sampler` have the same schema name, but
# the metrics in them are different (hence the differences in the schema
# digests). `samp-1/record_sampler` and `samp-2/record_sampler` have the same
# schema definition.
#
# The Flex Decomposition configuration in this test has the following mapping:
# - `samp-1/test_sampler` maps to a "test" static decomposition which produces
#   the following rows:
#   - `filter` (select some metrics)
#   - `fill` (select some metrics, and has a metric that does not exist)
# - `samp-1/record_sampler` and `samp-2/record_sampler` maps to:
#   - "record" static decomposition, producing "record" rows
#   - "as_is" as_is decomposition, producing "record_sampler_e1f021f" rows
#
# The `samp-2/test_sampler` does not have a specific digest mapping, so it will
# be matched with the default "*" digest mapping, which maps to "as_is"
# decomposition -- producing "test_sampler_95772b6" rows.
#
# See `conf/flex.json` for the Flex Decomposition configuration used for this
# test.

import os
import re
import pwd
import sys
import json
import time
import argparse
import TADA
import logging
import atexit

from distutils.spawn import find_executable
from LDMS_Test import LDMSDCluster, LDMSDContainer, process_args, \
                      add_common_args, jprint, parse_ldms_ls

if __name__ != "__main__":
    raise RuntimeError("This should not be impoarted as a module.")

class Debug(object):
    pass
D = Debug()

logging.basicConfig(format = "%(asctime)s %(name)s %(levelname)s %(message)s",
                    level = logging.INFO)

log = logging.getLogger(__name__)

exec(open(os.getenv("PYTHONSTARTUP", "/dev/null")).read())

#### default values #### ---------------------------------------------
sbin_ldmsd = find_executable("ldmsd")
if sbin_ldmsd:
    default_prefix, a, b = sbin_ldmsd.rsplit('/', 2)
else:
    default_prefix = "/opt/ovis"

#### argument parsing #### -------------------------------------------
ap = argparse.ArgumentParser(description = "LDMSD decomposition test")
add_common_args(ap)
args = ap.parse_args()
process_args(args)

#### config variables #### ------------------------------
USER = args.user
PREFIX = args.prefix
COMMIT_ID = args.commit_id
SRC = args.src
CLUSTERNAME = args.clustername
DB = args.data_root
LDMSD_PORT = 411
STORE_ROOT = "/store" # path inside container (agg-2)

SAMP1_TEST_DIGEST_STR = "8D2B8BD27CD9C888010D9B24F52F8E80E4AA2EAB2AD8690D7845902F2B2CDA12"
SAMP2_TEST_DIGEST_STR = "95772B66C5BCDAFE2C40E3B60063258DB0E13F4E235F0F26F97F42FFBB986ED4"
SAMP1_RECORD_DIGEST_STR = "E1F021F9273566F3675C1378CD29C6A3769AEE4178C198704E4D6A9130065F2B"
SAMP2_RECORD_DIGEST_STR = SAMP1_RECORD_DIGEST_STR

def __strgp_cfg(plugin, decomp, decomp_json, cont):
    cfg = [
        f"strgp_add name={plugin}_{decomp} plugin={plugin}"
        f" decomposition=/tada-src/conf/{decomp_json}"
        f" container={cont} regex=.*",
        f"strgp_prdcr_add name={plugin}_{decomp} regex=.*",
        f"strgp_start name={plugin}_{decomp}",
    ]
    return cfg

def strgp_cfg():
    tbl = {
            "store_kafka": [
                [ "flex", "flex.json", "kafka:9092" ],
            ],
            "store_sos": [
                [ "flex", "flex.json", "cont" ],
            ],
            "store_csv": [
                [ "flex", "flex.json", "cont" ],
            ],
        }
    ret = list()
    for k, lst in tbl.items():
        for p in lst:
            ret += __strgp_cfg(k, *p)
    return ret


#### spec #### -------------------------------------------------------
common_plugin_config = [
        "component_id=%component_id%",
        "instance=%hostname%/%plugin%",
        "producer=%hostname%",
    ]
spec = {
    "name" : CLUSTERNAME,
    "description" : "{}'s ldmsd_decomp_test cluster".format(USER),
    "type" : "NA",
    "INTERVAL" : 1000000,
    "templates" : { # generic template can apply to any object by "!extends"
        "compute-node" : {
            "daemons" : [
                {
                    "name" : "sshd", # for debugging
                    "type" : "sshd",
                },
                {
                    "name" : "sampler-daemon",
                    "!extends" : "ldmsd-sampler",
                },
            ],
        },
        "sampler_plugin" : {
            "interval" : "%INTERVAL%",
            "offset" : 0,
            "config" : common_plugin_config,
            "start" : True,
        },
        "ldmsd-base" : {
            "type" : "ldmsd",
            "listen" : [
                { "port" : LDMSD_PORT, "xprt" : "sock" },
            ],
        },
        "ldmsd-sampler" : {
            "!extends" : "ldmsd-base",
            "samplers" : [
                {
                    "plugin" : "record_sampler",
                    "!extends" : "sampler_plugin",
                },
            ],
            "config" : [
                # additional config for test_sampler
                "load name=test_sampler",
                "config name=test_sampler action=add_schema schema=test_sampler"
                "       metrics=%test_sampler_metrics%",
                "config name=test_sampler"
                "       action=add_set schema=test_sampler producer=%hostname%"
                "       instance=%hostname%/test_sampler"
                "       component_id=%component_id%",
                "start name=test_sampler interval=%INTERVAL% offset=0",
            ],
        },
        "prdcr" : {
            "host" : "%name%",
            "port" : LDMSD_PORT,
            "xprt" : "sock",
            "type" : "active",
            "interval" : "%INTERVAL%",
        },
        "ldmsd-aggregator" : {
            "!extends" : "ldmsd-base",
            "config" : [ # additional config applied after prdcrs
                "prdcr_start_regex regex=.*",
                "updtr_add name=all interval=1000000 offset=%offset%",
                "updtr_prdcr_add name=all regex=.*",
                "updtr_start name=all"
            ],
        },
    }, # templates
    "nodes" : [
        {
            "hostname" : "samp-1",
            "component_id" : 1,
            "test_sampler_metrics" :
                "component_id:m:u64:0,job_id:d:u64:0,app_id:d:u64:0,count:d:u64:0"
                ",u16:d:u16:0,u32:d:u32:0,u32_array:d:u32_array:0:8",
            "!extends" : "compute-node",
        },
        {
            "hostname" : "samp-2",
            "component_id" : 2,
            "test_sampler_metrics" :
                "component_id:m:u64:0,job_id:d:u64:0,app_id:d:u64:0,count:d:u64:0"
                ",u16:d:u16:0,u32_array:d:u32_array:0:8",
            "!extends" : "compute-node",
        }
    ] + [
        {
            "hostname" : f"agg-1{i}",
            "i" : i,
            "!extends" : "compute-node",
            "daemons" : [
                {
                    "name" : "sshd",
                    "type" : "sshd",
                },
                {
                    "name" : "agg",
                    "!extends" : "ldmsd-aggregator",
                    "offset" : 200000,
                    "prdcrs" : [ # these producers will turn into `prdcr_add`
                        {
                            "name" : "samp-%i%",
                            "!extends" : "prdcr",
                        }
                    ],
                },
            ]
        } for i in [1, 2]
    ] + [
        {
            "hostname" : "agg-2",
            "daemons" : [
                {
                    "name" : "sshd",
                    "type" : "sshd",
                },
                {
                    "name" : "aggregator",
                    "!extends" : "ldmsd-aggregator",
                    "offset" : 400000,
                    "prdcrs" : [ # these producers will turn into `prdcr_add`
                        {
                            "name" : f"agg-1{i}",
                            "!extends" : "prdcr",
                        } for i in [1,2]
                    ],
                    "config" : [
                        # additional config, mainly storages
                        # load storages
                        f"load name=store_kafka",
                        f"config name=store_kafka",
                        f"load name=store_sos",
                        f"config name=store_sos path=/{STORE_ROOT}/sos",
                        f"load name=store_csv",
                        f"config name=store_csv path=/{STORE_ROOT}/csv"
                        f" buffer=0",
                    ] + strgp_cfg() + [
                        # updtr
                        "prdcr_start_regex regex=.*",
                        "updtr_add name=all interval=%INTERVAL% offset=%offset%",
                        "updtr_prdcr_add name=all regex=.*",
                        "updtr_start name=all",
                    ],
                },
            ],
        },
        {
            "hostname" : "kafka",
            "daemons" : [
                {
                    "name" : "sshd",
                    "type" : "sshd",
                },
            ],
        },
        {
            "hostname" : "headnode",
            "daemons" : [
                {
                    "name" : "sshd",
                    "type" : "sshd",
                },
            ]
        },
    ], # nodes

    "cap_add": [ "SYS_PTRACE", "SYS_ADMIN" ],
    "image": args.image,
    "ovis_prefix": PREFIX,
    "env" : {
        "FOO": "BAR",
        "KAFKA_OPTS": "-Djava.net.preferIPv4Stack=True",
        "MALLOC_PERTURB_": "0x11",
        "MALLOC_CHECK_": "1",
    },
    "mounts": [
        f"{DB}:/db:rw",
        f"{DB}/tmp:/tmp:rw",
        f"{os.path.realpath(sys.path[0])}:/tada-src:ro",
    ] + args.mount +
    ( [f"{SRC}:{SRC}:ro"] if SRC else [] ),
}

#### test definition ####

#   samp-1/test_sampler
#   "8D2B8BD27CD9C888010D9B24F52F8E80E4AA2EAB2AD8690D7845902F2B2CDA12"
#   samp-1/record_sampler
#   "E1F021F9273566F3675C1378CD29C6A3769AEE4178C198704E4D6A9130065F2B"
#   samp-2/test_sampler
#   "95772B66C5BCDAFE2C40E3B60063258DB0E13F4E235F0F26F97F42FFBB986ED4"
#   samp-2/record_sampler
#   "E1F021F9273566F3675C1378CD29C6A3769AEE4178C198704E4D6A9130065F2B"#

ROW_SCHEMAS = [ 'test_sampler_95772b6',
                'record_sampler_e1f021f',
                'fill', 'filter', 'record',
                'meta_test', 'meta_record' ]
STORES = [ 'sos', 'csv', 'kafka' ]

test = TADA.Test(test_suite = "LDMSD",
                 test_type = "FVT",
                 test_name = "ldmsd_flex_decomp_test",
                 test_desc = "Flex decomposition test",
                 test_user = args.user,
                 commit_id = COMMIT_ID,
                 tada_addr = args.tada_addr)

# Test assertion
i = 1
for st in STORES:
    for  rs in ROW_SCHEMAS:
        test.add_assertion(i, f"{rs} {st} schema check")
        i += 1

for st in STORES:
    for rs in ROW_SCHEMAS:
        test.add_assertion(i, f"{rs} {st} data check")
        i += 1

#### Helper Functions ####
def ldms_ls(host, port = LDMSD_PORT, l = False):
    try:
        args = "-l -v" if l else ""
        rc, out = headnode.exec_run("bash -c 'ldms_ls {args} -x sock -p {port}" \
                                    "     -h {host} 2>/dev/null'" \
                                    .format(host=host, port=port, args=args))
        if l:
            return parse_ldms_ls(out)
        else:
            return out.splitlines()
    except:
        return None

#### Start! ####
cluster = None
test.start()

@atexit.register
def at_exit():
    rc = test.finish()
    if cluster is not None:
        cluster.remove()
    os._exit(rc)

os.makedirs(f"{DB}/tmp", mode=0o755, exist_ok=True)

log.info("-- Get or create the cluster --")
cluster = LDMSDCluster.get(spec["name"], create = True, spec = spec)

headnode = cluster.get_container("headnode")
agg2 = cluster.get_container("agg-2")
agg11 = cluster.get_container("agg-11")
agg12 = cluster.get_container("agg-12")
samp1 = cluster.get_container("samp-1")
samp2 = cluster.get_container("samp-2")
kafka = cluster.get_container("kafka")

headnode.exec_run("rm -rf /db/tmp/* ; "
                  "for X in /db/* ; do "
                  "  [[ ${X} == /db/tmp ]] || rm -rf ${X} ; "
                  "done")


agg2.exec_run("mkdir -p /store/{sos,csv}")

log.info("-- Start daemons --")

# get kafka directory
rc, out = kafka.exec_run("ls -d /opt/kafka*/")
assert(rc == 0)
out = out.splitlines()
assert(len(out) == 1)
KFK_DIR = out[0]
KFK_LOG_DIR = "/db/kafka_logs"
rc, out = kafka.exec_run(f"mkdir {KFK_LOG_DIR}")

# start zookeeper
ZOO_BIN = f"{KFK_DIR}/bin/zookeeper-server-start.sh"
ZOO_CONF = f"{KFK_DIR}/config/zookeeper.properties"
zoo_cmd = f"LOG_DIR={KFK_LOG_DIR} {ZOO_BIN} -daemon {ZOO_CONF}"
rc, out = kafka.exec_run(zoo_cmd)
assert(rc == 0)

rc, out = kafka.exec_run("/tada-src/python/zoo_check.py >/db/zoo_check.log 2>&1")
assert(rc == 0)

# start kafka
KFK_BIN = f"{KFK_DIR}/bin/kafka-server-start.sh"
KFK_CONF = f"{KFK_DIR}/config/server.properties"
kfk_cmd = f"LOG_DIR={KFK_LOG_DIR} {KFK_BIN} -daemon {KFK_CONF}"
rc, out = kafka.exec_run(kfk_cmd)
assert(rc == 0)

# start daemons defined in the spec
cluster.start_daemons()
cluster.make_known_hosts()

log.info("... wait a bit to make sure ldmsd's are up")
time.sleep(10)

def kafka_topic_verify(topics):
    topics = set(topics)
    S = set(["fill", "filter", "record", "meta_record", "meta_test"])
    if not S <= topics:
        return False
    topics -= S
    R = re.compile(r"^(record|test)_sampler_[0-9a-f]+$")
    r = 0
    t = 0
    # one record_sampler, two test_sampler
    for topic in topics:
        m = R.match(topic)
        if not m:
            return False
        (k,) = m.groups()
        if k == "record":
            r += 1
        else:
            t += 1
    if r != 1 or t != 1:
        return False
    return True

retry = 5
while retry:
    time.sleep(1)
    rc, out = kafka.exec_run(f"/tada-src/scripts/kafka-topic --list")
    if rc == 0:
        topics = out.splitlines()
        if kafka_topic_verify(topics):
            break;
    retry -= 1
else:
    raise RuntimeError(f"kafka-topic verification error, rc: {rc}, out: {out}")
for topic in topics:
    rc, out = kafka.exec_run(f"/bin/bash -c '/tada-src/scripts/kafka-console-consumer --topic {topic} > /db/kafka-{topic}.dump & '")

# Let it run for a while
time.sleep(10)

schemas = list(ROW_SCHEMAS)

# kill ldmsd on agg-2 to save space consumed by the growing stores
agg2.exec_run("pkill ldmsd")

agg2.exec_run("mkdir -p /db/rows")

def decomp_out_parse(out):
    lines = out.splitlines()
    objs = [ json.loads(l) for l in lines ]
    return { "schema": objs[0], "rows": objs[1:] }

# convert store data into rows
data = { "kafka": dict(), "sos": dict(), "csv": dict() }
for sch in schemas:
    cont = "cont"
    # sos
    rc, out = agg2.exec_run(
                f"/tada-src/python/decomp_out.py --store sos"
                f" --path /store/sos/{cont} --schema {sch}"
            )
    assert( rc == 0 )
    data["sos"][sch] = decomp_out_parse(out)
    # csv
    rc, out = agg2.exec_run(
                f"/tada-src/python/decomp_out.py --store csv"
                f" --path /store/csv/{cont}/{sch}"
            )
    assert( rc == 0 )
    data["csv"][sch] = decomp_out_parse(out)
    # kafka
    rc, out = kafka.exec_run(
                f"/tada-src/python/decomp_out.py --store kafka"
                f" --path /db/kafka-{sch}.dump"
            )
    assert( rc == 0 )
    data["kafka"][sch] = decomp_out_parse(out)

expected_schemas = {
    "test_sampler_95772b6": {
        "cols": ["timestamp", "component_id", "job_id", "app_id", "count",
            "u16", "u32_array"],
        "indices": [
            {"name": "timestamp", "cols": ["timestamp"]},
            {"name": "time_comp", "cols": ["timestamp", "component_id"]}
        ]
    },
    "record_sampler_e1f021f": {
        "cols": ["timestamp", "component_id", "job_id",
            "app_id", "round", "device_list.LDMS_V_CHAR",
            "device_list.LDMS_V_U8", "device_list.LDMS_V_S8",
            "device_list.LDMS_V_U16", "device_list.LDMS_V_S16",
            "device_list.LDMS_V_U32", "device_list.LDMS_V_S32",
            "device_list.LDMS_V_U64", "device_list.LDMS_V_S64",
            "device_list.LDMS_V_F32", "device_list.LDMS_V_D64",
            "device_list.LDMS_V_CHAR_ARRAY", "device_list.LDMS_V_U8_ARRAY",
            "device_list.LDMS_V_S8_ARRAY", "device_list.LDMS_V_U16_ARRAY",
            "device_list.LDMS_V_S16_ARRAY", "device_list.LDMS_V_U32_ARRAY",
            "device_list.LDMS_V_S32_ARRAY", "device_list.LDMS_V_U64_ARRAY",
            "device_list.LDMS_V_S64_ARRAY", "device_list.LDMS_V_F32_ARRAY",
            "device_list.LDMS_V_D64_ARRAY", "device_array.LDMS_V_CHAR",
            "device_array.LDMS_V_U8", "device_array.LDMS_V_S8",
            "device_array.LDMS_V_U16", "device_array.LDMS_V_S16",
            "device_array.LDMS_V_U32", "device_array.LDMS_V_S32",
            "device_array.LDMS_V_U64", "device_array.LDMS_V_S64",
            "device_array.LDMS_V_F32", "device_array.LDMS_V_D64",
            "device_array.LDMS_V_CHAR_ARRAY", "device_array.LDMS_V_U8_ARRAY",
            "device_array.LDMS_V_S8_ARRAY", "device_array.LDMS_V_U16_ARRAY",
            "device_array.LDMS_V_S16_ARRAY", "device_array.LDMS_V_U32_ARRAY",
            "device_array.LDMS_V_S32_ARRAY", "device_array.LDMS_V_U64_ARRAY",
            "device_array.LDMS_V_S64_ARRAY", "device_array.LDMS_V_F32_ARRAY",
            "device_array.LDMS_V_D64_ARRAY"],
        "indices": [
            {"name": "timestamp", "cols": ["timestamp"]},
            {"name": "time_comp", "cols": ["timestamp", "component_id"]}
        ]
    },
    "fill": {
        "cols": ["ts", "comp_id", "ushort", "uint", "ushort_array", "uint_array"],
        "indices": [
            {"name": "ts", "cols": ["ts"]},
            {"name": "time_comp", "cols": ["ts", "comp_id"]}
        ],
        "fills": [
            None, None, 16, 32,
            [16,16,16,16,16,16,16,16],
            [32,32,32,32,32,32,32,32]
        ]
    },
    "filter": {
        "cols": ["ts", "prdcr", "inst", "count", "comp_id", "ushort", "uint_array"],
        "indices": [
            {"name": "ts", "cols": ["ts"]},
            {"name": "time_comp", "cols": ["ts", "comp_id"]}
        ]
    },
    "record": {
        "cols": ["ts", "comp_id", "round", "dev.LDMS_V_U64", "dev.LDMS_V_U64_ARRAY"],
        "indices": [
            {"name": "ts", "cols": ["ts"]},
            {"name": "time_comp", "cols": ["ts", "comp_id"]}
        ]
    },
    "meta_test": {
        "cols": [
            "ts", "comp_id", "M_card", "M_digest", "M_duration", "M_gid",
            "M_instance", "M_perm", "M_producer", "M_schema", "M_uid", "u16"
        ],
        "indices": [
            {"name": "ts", "cols": ["ts"]},
            {"name": "time_comp", "cols": ["ts", "comp_id"]}
        ]
    },
    "meta_record": {
        "cols": [
            "ts", "comp_id", "M_card", "M_digest", "M_duration", "M_gid",
            "M_instance", "M_perm", "M_producer", "M_schema", "M_uid", "round"
        ],
        "indices": [
            {"name": "ts", "cols": ["ts"]},
            {"name": "time_comp", "cols": ["ts", "comp_id"]}
        ]
    },
}

COL_RE = re.compile(r'([^.]+)(?:\.(.+))?')

def row_to_obj(cols, row):
    # convert a row into dict object
    obj = dict()
    for c, v in zip(cols, row):
        a, b = COL_RE.match(c).groups()
        if b is not None:
            # record
            o = obj.setdefault(a, dict())
            o[b] = v
        else:
            obj[a] = v
    # put record into a list
    ret = { k: [v] if type(v) is dict else v for k,v in obj.items() }
    return ret

def data_to_snaps(_data):
    # converts rows into set snapshots
    cols = _data["schema"]["cols"]
    rows = _data["rows"]
    snaps = dict() # key := (ts, comp_id) or (timestamp, component_id)
    ts_col = -1
    comp_col = -1
    i = 0
    for c in cols:
        if c in [ "Time", "ts", "timestamp" ]:
            ts_col = i
        if c in [ "component_id", "comp_id" ]:
            comp_col = i
        i += 1
    assert( ts_col   > -1 )
    assert( comp_col > -1 )
    for row in rows:
        key = ( row[ts_col], row[comp_col] )
        obj = row_to_obj(cols, row)
        o = snaps.get(key)
        if o is None:
            snaps[key] = obj
        else:
            assert( set(o.keys()) == set(obj.keys()) )
            for k in o.keys():
                v0 = o[k]
                v1 = obj[k]
                if type(v0) == list:
                    assert(type(v1) == list)
                    if v0[-1] != v1[0]:
                        v0 += v1
    return snaps

def test_sampler_snap_check(snaps, assert_id, exp_comp_ids):
    comp_ids = set()
    for s in snaps.values():
        comp_id = s.get("component_id", s.get("comp_id"))
        if comp_id not in exp_comp_ids:
            test.assert_test(assert_id, False, f"Unexpected comp_id: {comp_id}")
            return False
        comp_ids.add( comp_id )
        a = s["count"]
        arr = [ a ] * 8
        for k, v in s.items():
            if k in [ "Time", "timestamp", "component_id", "job_id", "ts", "comp_id" ]:
                continue # skip
            if k == "inst":
                exp = f"samp-{comp_id}/test_sampler"
                if v != exp:
                    test.assert_test(assert_id, False, f"Expecting {exp}, but got {v}")
                    return False
                continue
            if k == "prdcr":
                exp = f"samp-{comp_id}"
                if v != exp:
                    test.assert_test(assert_id, False, f"Expecting {exp}, but got {v}")
                    return False
                continue
            if type(v) == list:
                if v != arr:
                    test.assert_test(assert_id, False, f"Expecting {arr}, but got {v}")
                    return False
            else:
                if v != a:
                    test.assert_test(assert_id, False, f"Expecting {a}, but got {v}")
                    return False
    if comp_ids != exp_comp_ids:
        test.assert_test(assert_id, False,
                f"Expecting {exp_comp_ids} component IDs, but got {comp_ids}")
        return False
    test.assert_test(assert_id, True, "OK")
    return True

def test_sampler_95772b6_snap_check(snaps, assert_id):
    return test_sampler_snap_check(snaps, assert_id, set([2]))

def rec_gen(i):
    COUNT = 8
    rec = dict()
    rec['LDMS_V_CHAR'] = 'a' if i % 2 == 0 else 'b'
    rec['LDMS_V_U8'] =   i % 256
    rec['LDMS_V_S8'] = -(i % 128)
    rec['LDMS_V_U16'] =   i + 1000
    rec['LDMS_V_S16'] = -(i + 1000)
    rec['LDMS_V_U32'] =   i + 100000
    rec['LDMS_V_S32'] = -(i + 100000)
    rec['LDMS_V_U64'] =   i + 200000
    rec['LDMS_V_S64'] = -(i + 200000)
    rec['LDMS_V_F32'] = float(i)
    rec['LDMS_V_D64'] = float(i)
    rec['LDMS_V_CHAR_ARRAY'] = f"a_{i}"
    rec['LDMS_V_U8_ARRAY'] = [  (i + j)%256 for j in range(0, COUNT) ]
    rec['LDMS_V_S8_ARRAY'] = [ -((i + j)%128) for j in range(0, COUNT) ]
    rec['LDMS_V_U16_ARRAY'] = [  (i + j + 1000) for j in range(0, COUNT) ]
    rec['LDMS_V_S16_ARRAY'] = [ -(i + j + 1000) for j in range(0, COUNT) ]
    rec['LDMS_V_U32_ARRAY'] = [  (i + j + 100000) for j in range(0, COUNT) ]
    rec['LDMS_V_S32_ARRAY'] = [ -(i + j + 100000) for j in range(0, COUNT) ]
    rec['LDMS_V_U64_ARRAY'] = [  (i + j + 500000) for j in range(0, COUNT) ]
    rec['LDMS_V_S64_ARRAY'] = [ -(i + j + 500000) for j in range(0, COUNT) ]
    rec['LDMS_V_F32_ARRAY'] = [  (i + j + 0.5 ) for j in range(0, COUNT) ]
    rec['LDMS_V_D64_ARRAY'] = [  (i + j + 0.75) for j in range(0, COUNT) ]
    return rec

def rec_static_gen(i):
    COUNT = 8
    rec = dict()
    rec['LDMS_V_U64'] =   i + 200000
    rec['LDMS_V_U64_ARRAY'] = [  (i + j + 500000) for j in range(0, COUNT) ]
    return rec


def gen_record_sampler_e1f021f(ts, comp_id, _round):
    N = 3
    obj = dict()
    obj["timestamp"] = ts
    obj["component_id"] = comp_id
    obj["job_id"] = 0
    obj["app_id"] = 0
    obj["round"] = _round
    obj["device_list"] = [ rec_gen(_round + i) for i in range(0, N) ]
    obj["device_array"] = [ rec_gen(_round + i + N) for i in range(0, N) ]
    return obj

def gen_record_static(ts, comp_id, _round):
    N = 3
    obj = dict()
    obj["ts"] = ts
    obj["comp_id"] = comp_id
    obj["round"] = _round
    obj["dev"] = [ rec_static_gen(_round + i) for i in range(0, N) ]
    return obj

def record_sampler_e1f021f_snap_check(snaps, assert_id):
    for s in snaps.values():
        ts = s.pop("timestamp", s.pop("Time", None))
        s["timestamp"] = ts
        comp_id = s.get("component_id")
        rnd = s.get("round")
        smp = gen_record_sampler_e1f021f(ts, comp_id, rnd)
        if smp.keys() != s.keys():
            test.assert_test(assert_id, False, f"Expecting keys {smp.keys()} but got {s.keys()}")
            return False
        for k in smp:
            v0 = smp[k]
            v1 = s[k]
            if type(v1) == list and type(v1[0]) == dict:
                # list of records
                for v in v1:
                    a = v["LDMS_V_S8_ARRAY"]
                    v["LDMS_V_S8_ARRAY"] = [ x if x < 128 else x - 256 for x in a ]
                    a = v["LDMS_V_S16"]
                    v["LDMS_V_S16"] = a if a < 32768 else a - 65536
                    a = v["LDMS_V_CHAR"]
                    if type(a) == int:
                        v["LDMS_V_CHAR"] = bytes([a]).decode()
            if v0 != v1:
                test.assert_test(assert_id, False, f"Expecting {v0} but got {v1}")
                return False
    test.assert_test(assert_id, True, "OK")
    return True

FILL = {
        "ushort": 16,
        "uint": 32,
        "ushort_array": [16,16,16,16,16,16,16,16],
        "uint_array": [32,32,32,32,32,32,32,32],
        }
def fill_snap_check(snaps, assert_id):
    comp_ids = set()
    for s in snaps.values():
        comp_id = s["comp_id"]
        if comp_id not in [1, 2]:
            test.assert_test(assert_id, False, f"Unexpected comp_id: {comp_id}")
            return False
        comp_ids.add( comp_id )
        a = s["ushort"]
        arr = [ a ] * 8
        for k, v in s.items():
            if k in ["comp_id", "ts", "Time"]:
                continue
            f = FILL[k]
            if type(f) == list:
                if v != arr and v != f:
                    test.assert_test(assert_id, False, f"Expecting {arr} or {f}, but got {v}")
                    return False
            else:
                if v != a and v != f:
                    test.assert_test(assert_id, False, f"Expecting {a} or {f}, but got {v}")
                    return False
    if comp_ids != set( [1] ):
        test.assert_test(assert_id, False,
                f"Expecting {1, 2} component IDs, but got {comp_ids}")
        return False
    test.assert_test(assert_id, True, "OK")
    return True

def filter_snap_check(snaps, assert_id):
    return test_sampler_snap_check(snaps, assert_id, set([1]))

def record_snap_check(snaps, assert_id):
    for s in snaps.values():
        ts = s.pop("ts", s.pop("Time", None))
        s["ts"] = ts
        comp_id = s.get("comp_id")
        rnd = s.get("round")
        smp = gen_record_static(ts, comp_id, rnd)
        if smp.keys() != s.keys():
            test.assert_test(assert_id, False, f"Expecting keys {smp.keys()} but got {s.keys()}")
            return False
        for k in smp:
            v0 = smp[k]
            v1 = s[k]
            if v0 != v1:
                test.assert_test(assert_id, False, f"Expecting {v0} but got {v1}")
                return False
    test.assert_test(assert_id, True, "OK")
    return True

def gen_meta_test(ts, comp_id, duration, u16):
    obj = dict()
    obj["ts"] = ts
    obj["comp_id"] = comp_id
    obj["M_card"] = 7 if comp_id == 1 else 6
    obj["M_digest"] = SAMP1_TEST_DIGEST_STR if comp_id == 1 else \
                      SAMP2_TEST_DIGEST_STR
    obj["M_duration"] = duration
    obj["M_gid"] = 0
    obj["M_instance"] = f"samp-{comp_id}/test_sampler"
    obj["M_perm"] = 0o440
    obj["M_producer"] = f"samp-{comp_id}"
    obj["M_schema"] = "test_sampler"
    obj["M_uid"] = 0
    obj["u16"] = u16

    return obj

def meta_test_snap_check(snaps, assert_id):
    base_ts = [ None, None , None ]
    base_u16 = [ None, None, None ]
    # sort by time
    values = [ v for v in snaps.values() ]
    # cleanse the 'Time' / 'ts' value
    for s in values:
        ts = s.pop("ts", s.pop("Time", None))
        s["ts"] = ts
    values.sort(key = lambda s: s['ts'])
    # check
    for s in values:
        ts = s['ts']
        comp_id    = s.get("comp_id")
        M_card     = s.get("M_card")
        M_digest   = s.get("M_digest")
        M_duration = s.get("M_duration")
        M_gid      = s.get("M_gid")
        M_instance = s.get("M_instance")
        M_perm     = s.get("M_perm")
        M_producer = s.get("M_producer")
        M_schema   = s.get("M_schema")
        M_uid      = s.get("M_uid")
        u16        = s.get("u16")
        if base_u16[comp_id] is None:
            e_u16 = u16
            base_u16[comp_id] = u16
            base_ts[comp_id] = int(ts)
        else:
            e_u16 = base_u16[comp_id] + (int(ts) - base_ts[comp_id])

        e = gen_meta_test(ts, comp_id, M_duration, e_u16)

        if e.keys() != s.keys():
            test.assert_test(assert_id, False,
                             f"Expecting keys {e.keys()} but got {s.keys()}")
            return False
        for k in e:
            v0 = e[k]
            v1 = s[k]
            if v0 != v1:
                test.assert_test(assert_id, False, f"Expecting {v0} but got {v1}")
                return False

    test.assert_test(assert_id, True, "OK")
    return True

def gen_meta_record(ts, comp_id, duration, _round):
    obj = dict()
    obj["ts"] = ts
    obj["comp_id"] = comp_id
    obj["M_card"] = 7
    obj["M_digest"] = SAMP1_RECORD_DIGEST_STR if comp_id == 1 else \
                      SAMP2_RECORD_DIGEST_STR
    obj["M_duration"] = duration
    obj["M_gid"] = 0
    obj["M_instance"] = f"samp-{comp_id}/record_sampler"
    obj["M_perm"] = 0o440
    obj["M_producer"] = f"samp-{comp_id}"
    obj["M_schema"] = "record_sampler"
    obj["M_uid"] = 0
    obj["round"] = _round

    return obj

def meta_record_snap_check(snaps, assert_id):
    prev_round = [ None, None, None ]
    for s in snaps.values():
        ts = s.pop("ts", s.pop("Time", None))
        s["ts"] = ts
        comp_id    = s.get("comp_id")
        M_card     = s.get("M_card")
        M_digest   = s.get("M_digest")
        M_duration = s.get("M_duration")
        M_gid      = s.get("M_gid")
        M_instance = s.get("M_instance")
        M_perm     = s.get("M_perm")
        M_producer = s.get("M_producer")
        M_schema   = s.get("M_schema")
        M_uid      = s.get("M_uid")
        _round        = s.get("round")
        if prev_round[comp_id] is not None:
            e_round = prev_round[comp_id] + 1
        else:
            e_round = _round
        e = gen_meta_record(ts, comp_id, M_duration, e_round)
        if e.keys() != s.keys():
            test.assert_test(assert_id, False,
                             f"Expecting keys {e.keys()} but got {s.keys()}")
            return False
        for k in e:
            v0 = e[k]
            v1 = s[k]
            if v0 != v1:
                test.assert_test(assert_id, False, f"Expecting {v0} but got {v1}")
                return False
        prev_round[comp_id] = _round

    test.assert_test(assert_id, True, "OK")
    return True


row_check_tbl = {
    "test_sampler_95772b6":   test_sampler_95772b6_snap_check,
    "record_sampler_e1f021f": record_sampler_e1f021f_snap_check,
    "fill":                   fill_snap_check,
    "filter":                 filter_snap_check,
    "record":                 record_snap_check,
    "meta_test":              meta_test_snap_check,
    "meta_record":            meta_record_snap_check,
}

i = 1
for st in STORES:
    for rs in ROW_SCHEMAS:
        # test.add_assertion(i, f"{rs} {st} schema check")
        data_sch = data[st][rs]["schema"]
        exp_sch = expected_schemas[rs]
        while True: # will break
            exp_cols = list(exp_sch["cols"])
            data_cols = list(data_sch["cols"])
            if data_cols[0] not in ["Time", "ts", "timestamp"]:
                test.assert_test(i, False, f"Missing leading timesrtamp column")
                break
            exp_cols = exp_cols[1:]
            data_cols = data_cols[1:]
            if data_cols != exp_cols:
                test.assert_test(i, False, f"columns mismatch")
                break
            if st == "sos" and data_sch["indices"] != exp_sch["indices"]:
                test.assert_test(i, False, f"indices mismatch")
                break
            test.assert_test(i, True, f"OK")
            break
        i += 1

for st in STORES:
    for rs in ROW_SCHEMAS:
        # test.add_assertion(i, f"{rs} {st} data check")
        check_fn = row_check_tbl[rs]
        rows = data[st][rs]["rows"]
        snap = data_to_snaps(data[st][rs])
        check_fn(snap, i)
        i += 1

test.finish()
