#!/usr/bin/env python3

import os
import re
import sys
import pwd
import json
import time
import docker
import argparse
import TADA
import logging

from distutils.spawn import find_executable
from LDMS_Test import LDMSDCluster, LDMSDContainer, jprint, \
                      add_common_args, process_args

#if __name__ != "__main__":
#    raise RuntimeError("This should not be imported as a module.")

logging.basicConfig(format = "%(asctime)s %(name)s %(levelname)s %(message)s",
                    level = logging.INFO)

log = logging.getLogger(__name__)

exec(open(os.getenv("PYTHONSTARTUP", "/dev/null")).read())

def update_expect_file(fname, data):
    s = json.dumps(data)
    f = open(fname, 'w')
    f.write(s)
    f.close()

#### argument parsing #### -------------------------------------------
ap = argparse.ArgumentParser(description =
                         "Run test scenario of slurm stream using ldmsd_stream_publish " \
                         "with slurm data and job slot verification." )
add_common_args(ap)
args = ap.parse_args()
process_args(args)

#### config variables #### ------------------------------
USER = args.user
PREFIX = args.prefix
COMMIT_ID = args.commit_id
SRC = args.src
CLUSTERNAME = args.clustername
DB = args.data_root

#### spec #### -------------------------------------------------------
spec = {
    "name" : CLUSTERNAME,
    "description" : "{}'s slurm stream test cluster".format(USER),
    "type" : "NA",
    "templates" : {
        "compute-node" : {
            "daemons" : [
                {
                    "name" : "munged",
                    "type" : "munged",
                },
                {
                    "name" : "sampler-daemon",
                    "requires" : [ "munged" ],
                    "!extends" : "ldmsd-sampler",
                }
            ],
        },
        "ldmsd-base" : {
            "type" : "ldmsd",
            "listen_port" : 10000,
            "listen_xprt" : "sock",
            "listen_auth" : "munge",
        },
        "ldmsd-sampler" : {
            "!extends" : "ldmsd-base",
            "samplers" : [
                {
                    "plugin" : "slurm_sampler",
                    "interval" : 1000000,
                    "offset" : 0,
                    "config" : [
                        "component_id=%component_id%",
                        "stream=test-slurm-stream",
                        "instance=%hostname%/%plugin%",
                        "producer=%hostname%",
                        "task_count=16",
                    ],
                    "start" : True,
                }
            ],
        },
        "prdcr" : {
            "host" : "%name%",
            "port" : 10000,
            "xprt" : "sock",
            "type" : "active",
            "interval" : 1000000,
        },
    },
    "nodes" : [
        {
            "hostname" : "compute-1",
            "component_id" : 10001,
            "!extends" : "compute-node",
        },
        {
            "hostname" : "compute-2",
            "component_id" : 10002,
            "!extends" : "compute-node",
        },
    ],

    "cap_add": [ "SYS_PTRACE" ],
    "image": "ovis-centos-build",
    "ovis_prefix": PREFIX,
    "env" : { "FOO": "BAR" },
    "mounts": [
        "{}:/db:rw".format(DB),
    ] +
    ( ["{0}:{0}:ro".format(SRC)] if SRC else [] ),
}


#### test definition ####

test = TADA.Test(test_suite = "LDMSD",
                 test_type = "LDMSD",
                 test_name = "slurm stream test",
                 test_user = args.user,
                 commit_id = COMMIT_ID,
                 tada_addr = args.tada_addr)

test.add_assertion(1, "Job properly assigned to correct slot")
test.add_assertion(2, "job_start correctly represented in metric set")
test.add_assertion(3, "job_end correctly represented in metric set")
test.add_assertion(4, "task_pid correctly represented")
test.add_assertion(5, "task_pid correctly represented")
test.add_assertion(6, "task_pid correctly represented")
test.add_assertion(7, "task_pid correctly represented")
test.add_assertion(8, "Job properly assigned to correct slot")
test.add_assertion(9, "job_start correctly represented in metric set")
test.add_assertion(10, "job_end correctly represented in metric set")
test.add_assertion(11, "task_pid correctly represented")
test.add_assertion(12, "task_pid correctly represented")
test.add_assertion(13, "task_pid correctly represented")
test.add_assertion(14, "task_pid correctly represented")
test.add_assertion(15, "Job properly assigned to correct slot")
test.add_assertion(16, "job_start correctly represented in metric set")
test.add_assertion(17, "job_end correctly represented in metric set")
test.add_assertion(18, "task_pid correctly represented")
test.add_assertion(19, "task_pid correctly represented")
test.add_assertion(20, "task_pid correctly represented")
test.add_assertion(21, "task_pid correctly represented")
test.add_assertion(22, "Job properly assigned to correct slot")
test.add_assertion(23, "job_start correctly represented in metric set")
test.add_assertion(24, "job_end correctly represented in metric set")
test.add_assertion(25, "task_pid correctly represented")
test.add_assertion(26, "task_pid correctly represented")
test.add_assertion(27, "task_pid correctly represented")
test.add_assertion(28, "task_pid correctly represented")
test.add_assertion(29, "Job properly assigned to correct slot")
test.add_assertion(30, "job_start correctly represented in metric set")
test.add_assertion(31, "job_end correctly represented in metric set")
test.add_assertion(32, "task_pid correctly represented")
test.add_assertion(33, "task_pid correctly represented")
test.add_assertion(34, "task_pid correctly represented")
test.add_assertion(35, "task_pid correctly represented")
test.add_assertion(36, "Job properly assigned to correct slot")
test.add_assertion(37, "job_start correctly represented in metric set")
test.add_assertion(38, "job_end correctly represented in metric set")
test.add_assertion(39, "task_pid correctly represented")
test.add_assertion(40, "task_pid correctly represented")
test.add_assertion(41, "task_pid correctly represented")
test.add_assertion(42, "task_pid correctly represented")
test.add_assertion(43, "Job properly assigned to correct slot")
test.add_assertion(44, "job_start correctly represented in metric set")
test.add_assertion(45, "job_end correctly represented in metric set")
test.add_assertion(46, "task_pid correctly represented")
test.add_assertion(47, "task_pid correctly represented")
test.add_assertion(48, "task_pid correctly represented")
test.add_assertion(49, "task_pid correctly represented")
test.add_assertion(50, "Job properly assigned to correct slot")
test.add_assertion(51, "job_start correctly represented in metric set")
test.add_assertion(52, "job_end correctly represented in metric set")
test.add_assertion(53, "task_pid correctly represented")
test.add_assertion(54, "task_pid correctly represented")
test.add_assertion(55, "task_pid correctly represented")
test.add_assertion(56, "task_pid correctly represented")

test.add_assertion(57, "Job properly assigned to correct slot")
test.add_assertion(58, "job_start correctly represented in metric set")
test.add_assertion(59, "job_end correctly represented in metric set")
test.add_assertion(60, "task_pid correctly represented")
test.add_assertion(61, "task_pid correctly represented")
test.add_assertion(62, "task_pid correctly represented")
test.add_assertion(63, "task_pid correctly represented")
test.add_assertion(64, "task_pid correctly represented")
test.add_assertion(65, "task_pid correctly represented")
test.add_assertion(66, "task_pid correctly represented")
test.add_assertion(67, "task_pid correctly represented")
test.add_assertion(68, "Job properly assigned to correct slot")
test.add_assertion(69, "job_start correctly represented in metric set")
test.add_assertion(70, "job_end correctly represented in metric set")
test.add_assertion(71, "task_pid correctly represented")
test.add_assertion(72, "task_pid correctly represented")
test.add_assertion(73, "task_pid correctly represented")
test.add_assertion(74, "task_pid correctly represented")
test.add_assertion(75, "task_pid correctly represented")
test.add_assertion(76, "task_pid correctly represented")
test.add_assertion(77, "task_pid correctly represented")
test.add_assertion(78, "task_pid correctly represented")
test.add_assertion(79, "Job properly assigned to correct slot")
test.add_assertion(80, "job_start correctly represented in metric set")
test.add_assertion(81, "job_end correctly represented in metric set")
test.add_assertion(82, "task_pid correctly represented")
test.add_assertion(83, "task_pid correctly represented")
test.add_assertion(84, "task_pid correctly represented")
test.add_assertion(85, "task_pid correctly represented")
test.add_assertion(86, "task_pid correctly represented")
test.add_assertion(87, "task_pid correctly represented")
test.add_assertion(88, "task_pid correctly represented")
test.add_assertion(89, "task_pid correctly represented")
test.add_assertion(90, "Job properly assigned to correct slot")
test.add_assertion(91, "job_start correctly represented in metric set")
test.add_assertion(92, "job_end correctly represented in metric set")
test.add_assertion(93, "task_pid correctly represented")
test.add_assertion(94, "task_pid correctly represented")
test.add_assertion(95, "task_pid correctly represented")
test.add_assertion(96, "task_pid correctly represented")
test.add_assertion(97, "task_pid correctly represented")
test.add_assertion(98, "task_pid correctly represented")
test.add_assertion(99, "task_pid correctly represented")
test.add_assertion(100, "task_pid correctly represented")
test.add_assertion(101, "Job properly assigned to correct slot")
test.add_assertion(102, "job_start correctly represented in metric set")
test.add_assertion(103, "job_end correctly represented in metric set")
test.add_assertion(104, "task_pid correctly represented")
test.add_assertion(105, "task_pid correctly represented")
test.add_assertion(106, "task_pid correctly represented")
test.add_assertion(107, "task_pid correctly represented")
test.add_assertion(108, "task_pid correctly represented")
test.add_assertion(109, "task_pid correctly represented")
test.add_assertion(110, "task_pid correctly represented")
test.add_assertion(111, "task_pid correctly represented")
test.add_assertion(112, "Job properly assigned to correct slot")
test.add_assertion(113, "job_start correctly represented in metric set")
test.add_assertion(114, "job_end correctly represented in metric set")
test.add_assertion(115, "task_pid correctly represented")
test.add_assertion(116, "task_pid correctly represented")
test.add_assertion(117, "task_pid correctly represented")
test.add_assertion(118, "task_pid correctly represented")
test.add_assertion(119, "task_pid correctly represented")
test.add_assertion(120, "task_pid correctly represented")
test.add_assertion(121, "task_pid correctly represented")
test.add_assertion(122, "task_pid correctly represented")
test.add_assertion(123, "Job properly assigned to correct slot")
test.add_assertion(124, "job_start correctly represented in metric set")
test.add_assertion(125, "job_end correctly represented in metric set")
test.add_assertion(126, "task_pid correctly represented")
test.add_assertion(127, "task_pid correctly represented")
test.add_assertion(128, "task_pid correctly represented")
test.add_assertion(129, "task_pid correctly represented")
test.add_assertion(130, "task_pid correctly represented")
test.add_assertion(131, "task_pid correctly represented")
test.add_assertion(132, "task_pid correctly represented")
test.add_assertion(133, "task_pid correctly represented")
test.add_assertion(134, "Job properly assigned to correct slot")
test.add_assertion(135, "job_start correctly represented in metric set")
test.add_assertion(136, "job_end correctly represented in metric set")
test.add_assertion(137, "task_pid correctly represented")
test.add_assertion(138, "task_pid correctly represented")
test.add_assertion(139, "task_pid correctly represented")
test.add_assertion(140, "task_pid correctly represented")
test.add_assertion(141, "task_pid correctly represented")
test.add_assertion(142, "task_pid correctly represented")
test.add_assertion(143, "task_pid correctly represented")
test.add_assertion(144, "task_pid correctly represented")



test.add_assertion(145, "new job correctly replaces oldest slot")
test.add_assertion(146, "new job_start correctly represented in metric set")
test.add_assertion(147, "new job_end correctly represented in metric set")
test.add_assertion(148, "new job's task replaces oldest slot")
test.add_assertion(149, "new job's task replaces oldest slot")
test.add_assertion(150, "new job's task replaces oldest slot")
test.add_assertion(151, "new job's task replaces oldest slot")
test.add_assertion(152, "new job's task replaces oldest slot")
test.add_assertion(153, "new job's task replaces oldest slot")
test.add_assertion(154, "new job's task replaces oldest slot")
test.add_assertion(155, "new job's task replaces oldest slot")

#test.add_assertion(156, "Job data in each metric set reflects matching text file")


#### Start! ####
test.start()

log.info("-- Get or create the cluster --")
cluster = LDMSDCluster.get(spec["name"], create = True, spec = spec)

log.info("-- Start daemons --")
cluster.start_daemons()

log.info("... wait a bit to make sure ldmsd's are up")
time.sleep(5)

def verify(num, cond, cond_str):
    test.assert_test(num, cond, cond_str)

json_path = args.data_root


### create slurm stream data ###
def create_events(node_count, task_count, job_id, init_time = None):
    """
    Generate events for node_count nodes. Each node will
    have total_tasks / node_count tasks

    The return value is an array of arrays as follows:
    node_events[node][event]

    node_events[node] is an array of events
    node_events[node][event] is  a dictionary
    """
    nodes = []
    if not init_time:
        init_time = int(time.time())
    exit_time = init_time + 5
    task_start = 0
    for node in range(0, node_count):
        events = []
        init_event = {}
        init_event['timestamp'] = init_time
        init_event['schema'] = 'mt-slurm'
        init_event['event'] = 'init'
        init_event['context'] = 'remote'
        init_event['data'] = {}
        init_event['data']['job_name'] = 'test'
        init_event['data']['job_id'] = job_id
        init_event['data']['subscriber_data'] = 'test'
        init_event['data']['uid'] = 0
        init_event['data']['gid'] = 0
        init_event['data']['nnodes'] = node_count
        init_event['data']['nodeid'] = node
        init_event['data']['local_tasks'] = int(task_count / node_count)
        init_event['data']['total_tasks'] = task_count
        events.append(init_event)
        for task in range(0, int(task_count / node_count)):
            init_task = {}
            init_task['schema'] = 'mt-slurm'
            init_task['event'] = 'task_init_priv'
            init_task['context'] = 'remote'
            init_task['timestamp'] = init_time
            init_task['data'] = {}
            init_task['data']['job_id'] = job_id
            init_task['data']['task_pid'] = task + job_id
            init_task['data']['task_id'] = task
            init_task['data']['task_global_id'] = task + task_start
            init_task['data']['nodeid'] = node
            events.append(init_task)
        for task in range(0, int(task_count / node_count)):
            exit_task = {}
            exit_task['schema'] = 'mt-slurm'
            exit_task['event'] = 'task_exit'
            exit_task['context'] = 'remote'
            exit_task['timestamp'] = exit_time
            exit_task['data'] = {}
            exit_task['data']['job_id'] = job_id
            exit_task['data']['task_pid'] = task + job_id
            exit_task['data']['task_id'] = task
            exit_task['data']['task_global_id'] = task + task_start
            exit_task['data']['nodeid'] = node
            exit_task['data']['task_exit_status'] = 0
            events.append(exit_task)
        exit_event = {}
        exit_event['schema'] = 'mt-slurm'
        exit_event['event'] = 'exit'
        exit_event['context'] = 'remote'
        exit_event['timestamp'] = exit_time
        exit_event['data'] = {}
        exit_event['data']['job_id'] = job_id
        exit_event['nodeid'] = node
        events.append(exit_event)
        nodes.append(events)
        task_start += int(task_count / node_count)
    return nodes

def deliver_events(events):
    print("Delivering events...")
    for node in events:
         node_id = node[0]['data']['nodeid'] + 1
         cont = cluster.get_container("compute-{0}".format(node_id))
         for event in node:
            update_expect_file(json_path+"/event-file.json", event)
            rc, out = cont.exec_run("ldmsd_stream_publish -h compute-{node} -x sock -p 10000"
                              " -a munge -s test-slurm-stream -t json -f {fname}"
                                .format(fname="/db/event-file.json",
                                        node=node_id))


### job for compute-1 ###

# Add job events over stream
i = 0
task_count = 8

data_file = "/db/Slurm_Test-data.json"
assert_num = 1

def test_jobs(node_count, jobs):
    global assert_num
    for job in range(0, len(jobs)):
        for nodeid in range(0, node_count):
            node_id = nodeid + 1
            cnt = 0
            task_inits = []
            job_inits = []
            job_exits = []
            events = list(jobs[job][nodeid])
            for i in range(0, len(events)):
                if events[i]['event'] == 'init':
                    job_inits.append(events[i])
                elif events[i]['event'] == 'task_init_priv':
                    task_inits.append(events[i])
                elif events[i]['event'] == 'exit':
                    job_exits.append(events[i])
            cid = nodeid+10001

            cont = cluster.get_container("compute-{node_id}".format(node_id=node_id))
            rc, out = cont.ldms_ls("-h compute-{node_id} -x sock -p 10000 -a munge -l"
                                   .format(node_id=node_id))
            cnt = 0
            k = 0
            for line in (out.split('\n')):
                if cnt == 2:
                    ids = line.split()[3].split(',')
                    verify(assert_num, int(ids[job]) == int(job_inits[0]['data']['job_id']),
                           'correct job_id fills next slot')
                    log.info(ids[job])
                    log.info(job_inits[0]['data']['job_id'])
                    assert_num += 1
                elif cnt == 10:
                    tstamp = line.split()[3].split(',')
                    verify(assert_num, int(tstamp[job]) == job_inits[0]['timestamp'],
                               'with mult jobs running for Job '+str(job_inits[0]['data']['job_id']))
                    assert_num += 1
                elif cnt == 11:
                    tstamp = line.split()[3].split(',')
                    verify(assert_num, int(tstamp[job]) == job_exits[0]['timestamp'],
                           'with mutl jobs running, for Job '+str(job_exits[0]['data']['job_id']))
                    assert_num += 1
                if cnt == 14+job:
                    i = 0
                    task_ids = line.split()[3].split(',')
                    for i in range(len(task_inits)):
                        verify(assert_num, int(task_ids[i]) == int(task_inits[i]['data']['task_pid']),
                               'with mult jobs running for Job '+str(job_inits[0]['data']['job_id']))
                        assert_num += 1
                    break
                cnt += 1

# array to store created jobs and nested events
jobs = []
j = 0

# add jobs to compute-1
t = int(time.time())
job_id = 12345
while j < 4:
    events_1_8 = create_events(2, 8, job_id, t)
    jobs.append(events_1_8)
    deliver_events(events_1_8)
    job_id += 1
    j += 1

job_id = 12355
j = 0
while j < 4:
    events_1_16 = create_events(2, 16, job_id, t)
    jobs.append(events_1_16)
    deliver_events(events_1_16)
    job_id += 1
    j += 1

test_jobs(2, jobs)

new_event_node1 = create_events(1, 8, 12353, t + 10)
deliver_events(new_event_node1)
jobs = [ new_event_node1 ]
test_jobs(1, jobs)

'''
test_results = open(json_path+"/Slurm_Test-results.txt", "w")
for host in ['compute-1', 'compute-2']:
    cont = cluster.get_container(host)
    rc, out = cont.ldms_ls("-h {host} -p 10000 -a munge -l".format(host=host))
    cnt = 0
    for line in out.split('\n'):
        if cnt == 0:
            pass
        else:
            test_results.write(line+'\n')
        cnt += 1

test_results.close()
results = open(json_path+'/Slurm_Test-results.txt').read()
expected_data = open(json_path+'/Slurm_Test-static.txt').read()
verify(156, (results == expected_data), 'ldms_ls == text_file')
'''
log.info("-- Test Finished --")

test.finish()

cluster.remove()
