#!/usr/bin/env python3
from __future__ import print_function
import os
import re
import sys
import pwd
import json
import time
import argparse
import logging
import atexit

import TADA

from LDMS_Test import LDMSDCluster, LDMSDContainer, \
                      add_common_args, process_args, cond_timedwait, \
                      debug_prompt

logging.basicConfig(format = "%(asctime)s %(name)s %(levelname)s %(message)s",
                    level = logging.INFO)

log = logging.getLogger(__name__)

USER = pwd.getpwuid(os.geteuid())[0]

CPU_PER_NODE = 4
NUM_NODES = 8
OVERSUBSCRIBE = "FORCE" # NO, YES, or FORCE -- see slurm cons_res_share

spec = {
    "name" : "TO-BE-REPLACED",
    "description" : "spank_notifier_test cluster",
    "type" : "NA",
    "templates" : {
        "compute-node" : {
            "daemons" : [
                {
                    "name" : "sshd",
                    "type" : "sshd",
                },
                {
                    "name" : "slurmd",
                    "!extends" : "slurmd-template",
                },
            ],
        },
        "slurmd-template" : {
            "type" : "slurmd",
            "plugstack" : [
                {
                    "required" : True,
                    "path" : "%libdir%/ovis-ldms/libslurm_notifier.so",
                    "args" : [
                        "auth=none",
                        "port=20000",
                        "timeout=1",
                        "client=sock:localhost:20000:none",
                    ],
                },
            ],
        },
    },
    "nodes" :
        [
            {
                "hostname": "node-{}".format(i),
                "!extends": "compute-node"
            } for i in range(1, NUM_NODES+1)
        ] + \
        [
            {
                "hostname" : "headnode",
                "daemons" : [
                    {
                        "name" : "sshd",
                        "type" : "sshd",
                    },
                    {
                        "name" : "slurmctld",
                        "type" : "slurmctld",
                    },
                ],
            },
        ],
    "cpu_per_node" : CPU_PER_NODE,
    "oversubscribe" : OVERSUBSCRIBE,
    "cap_add": [ "SYS_PTRACE" ],
    "image": "ovis-centos-build",
    "libdir": "/opt/ovis/lib64",
}

def make_dict_from_out(out):
    env = out.split('\n')
    d = {}
    for e in env:
        e = e.split('=')
        if len(e) > 1:
            d[e[0]] = e[1]
    return d

def remove_job_out(cluster):
    cont = cluster.containers[-1]
    rc, out = cont.exec_run("bash -c 'rm -f /db/slurm*.out'")
    rc, out = cont.exec_run("bash -c 'rm -f /db/spank*.out'")
    rc, out = cont.exec_run("bash -c 'rm -f /db/crash*.out'")

def get_mount_libdir(prefix, mount_prefix):
    if os.path.exists(prefix + "/lib64"):
        return mount_prefix + "/lib64"
    return mount_prefix + "/lib"

def submit_job(cluster, num_tasks):
    cont = cluster.get_container("headnode")
    cont.write_file('/db/show_spank_env.sh',
                    '#!/bin/bash\n'\
                    'sleep 5\n'\
                    'env | grep SLURM > /db/slurm_env_${SLURM_JOBID}.${SLURM_PROCID}.out\n')
    script_path = "/db/spank_job_{}.sh".format(num_tasks)
    cont.write_file(script_path,
                    '#!/bin/bash\n'\
                    '#SBATCH -n {num_tasks}\n'\
                    '#SBATCH -D /db\n'\
                    'export SUBSCRIBER_DATA=\'{{"sub":"data"}}\'\n'\
                    'srun /bin/bash /db/show_spank_env.sh\n'\
                    .format(
                        num_tasks = num_tasks,
                    ))
    log.info("-- Submitting job with num_tasks {} --".format(num_tasks))
    jobid = cluster.sbatch(script_path)
    log.info("  jobid = {0}".format(jobid))
    return jobid


def verify_jobinfo(cluster, test, node_events, jobinfo):
    jobid = jobinfo["jobid"]
    num_tasks = jobinfo["num_tasks"]
    assert_no = jobinfo["first_assertion"]
    subscriber_data = jobinfo["subscriber_data"]
    # get ENVs for each task
    envs = [ make_dict_from_out("/db/slurm_env_{}.{}.out".format(jobid, i)) \
                                for i in range(0, num_tasks) ]
    # filter events to only our job
    _node_events = [ list(filter(lambda e: e['data']['job_id'] == jobid, _events)) \
                    for _events in node_events ]
    _node_events = [ _events for _events in _node_events if len(_events) > 0 ]
    if not _node_events: # no events
        test.assert_test(assert_no, False, "No events")
        return

    # for each node, verify event order
    for job_events in _node_events:
        ev = job_events.pop(0)
        if ev['event'] != 'init':
            test.assert_test(assert_no, False, '{0} == {1}'.format(ev['event'], 'init'))
            return
        ev = job_events.pop(0)
        if ev['event'] != 'step_init':
            test.assert_test(assert_no + 1, False,
                             'Expecting "step_init" event, but got {0}' \
                             .format(ev['event']))
            return
        # check subsrciber_data
        if ev['data']['subscriber_data'] != subscriber_data:
            test.assert_test(assert_no + 1, False, '{0} == {1}'.format(ev['data']['subscriber_data'], subscriber_data))
            return
        num_tasks = ev['data']['local_tasks']
        prev_task_event = [None] * num_tasks
        exit_event = None
        # tasks
        for ev in job_events:
            task_id = ev['data'].get('task_id') # local task_id
            if ev['event'] == 'task_init_priv':
                prev = prev_task_event[task_id]
                if prev: # task_init_priv must be 1st
                    test.assert_test(assert_no + 2, False, "Unexpexted `task_init_priv`")
                    return
            elif ev['event'] == 'task_exit':
                prev = prev_task_event[task_id]
                if not prev or prev['event'] != 'task_init_priv':
                    test.assert_test(assert_no + 3, False, "Unexpected `task_exit`")
                    return
            elif ev['event'] == 'exit':
                exit_event = ev
            if task_id != None:
                prev_task_event[task_id] = ev
        for ev in prev_task_event: # all ev must be `task_exit`
            if not ev or ev["event"] != "task_exit":
                test.assert_test(assert_no + 3, False, "`task_exit` event missing")
                return
        if not exit_event:
            test.assert_test(assert_no + 4, False, "`exit` event missing")
            return
        for ev in prev_task_event:
            if ev['timestamp'] > exit_event['timestamp']:
                test.assert_test(assert_no + 4, False,
                                 "`exit` event precedes `task_exit`")
                return
    test.assert_test(assert_no,     True, "`init` verified")
    test.assert_test(assert_no + 1, True, "`init` subscriber_data verified")
    test.assert_test(assert_no + 2, True, "`task_init_priv` verified")
    test.assert_test(assert_no + 3, True, "`task_exit` verified")
    test.assert_test(assert_no + 4, True, "`exit` verified")

def undrain():
    cont = cluster.get_container("headnode")
    cont.exec_run("scontrol update node=node-[1-8] state=down reason=undrain")
    cont.exec_run("scontrol update node=node-[1-8] state=resume")
    time.sleep(10)


if __name__ == "__main__":
    if sys.flags.interactive:
        exec(open(os.getenv("PYTHONSTARTUP", "/dev/null")).read())
    parser = argparse.ArgumentParser(description="Run an FVT test")
    add_common_args(parser)
    parser.add_argument("--libdir",
                        help="The directory where the test target is installed.",
                        default="__find_from_prefix__")
    args = parser.parse_args()
    process_args(args)

    COMMIT_ID = args.commit_id
    spec["name"] = args.clustername
    spec["image"] = args.image

    test = TADA.Test(test_suite = "Slurm_Plugins",
                     test_type = "FVT",
                     test_name = "spank_notifier_test",
                     test_desc = "The test to verify ldmsd-stream SPANK notifier.",
                     test_user = args.user,
                     commit_id = COMMIT_ID,
                     tada_addr=args.tada_addr)
    test.add_assertion( 0, 'Missing stream listener on node-1 does not affect job execution')
    test.add_assertion( 1, 'Missing stream listener on node-2 does not affect job execution')

    test.add_assertion( 2, "1-task job: first event is 'init'")
    test.add_assertion( 3, "1-task job: 'step_init' event contains subscriber data")
    test.add_assertion( 4, "1-task job: second event is 'task_init_priv'")
    test.add_assertion( 5, "1-task job: third event is 'task_exit'")
    test.add_assertion( 6, "1-task job: fourth event is 'exit'")

    test.add_assertion( 7, "2-task job: first event is 'init'")
    test.add_assertion( 8, "2-task job: 'step_init' event contains subscriber data")
    test.add_assertion( 9, "2-task job: second event is 'task_init_priv'")
    test.add_assertion(10, "2-task job: third event is 'task_exit'")
    test.add_assertion(11, "2-task job: fourth event is 'exit'")

    test.add_assertion(12, "4-task job: first event is 'init'")
    test.add_assertion(13, "4-task job: 'step_init' event contains subscriber data")
    test.add_assertion(14, "4-task job: second event is 'task_init_priv'")
    test.add_assertion(15, "4-task job: third event is 'task_exit'")
    test.add_assertion(16, "4-task job: fourth event is 'exit'")

    test.add_assertion(17, "8-task job: first event is 'init'")
    test.add_assertion(18, "8-task job: 'step_init' event contains subscriber data")
    test.add_assertion(19, "8-task job: second event is 'task_init_priv'")
    test.add_assertion(20, "8-task job: third event is 'task_exit'")
    test.add_assertion(21, "8-task job: fourth event is 'exit'")

    test.add_assertion(22, "27-task job: first event is 'init'")
    test.add_assertion(23, "27-task job: 'step_init' event contains subscriber data")
    test.add_assertion(24, "27-task job: second event is 'task_init_priv'")
    test.add_assertion(25, "27-task job: third event is 'task_exit'")
    test.add_assertion(26, "27-task job: fourth event is 'exit'")

    test.add_assertion(50, "Multi-tenant verification")

    test.add_assertion(51, "Killing stream listener does not affect job execution on node-1")
    test.add_assertion(52, "Killing stream listener does not affect job execution on node-2")

    test.add_assertion(60, "Bad config does not affect jobs")

    cluster = None
    test.start()

    @atexit.register
    def at_exit():
        rc = test.finish()
        if cluster is not None:
            try:
                remove_job_out(cluster)
            except:
                pass
            cluster.remove()
        os._exit(rc)

    log.info("-- Create the cluster --")
    spec['ovis_prefix'] = args.prefix
    spec['mounts'] = [ args.data_root + ':/db:rw' ] + args.mount
    if args.src:
        spec['mounts'].append("{src}:{src}:rw".format(src=args.src))
    # modify slurm_notifier path in spec
    if args.libdir == "__find_from_prefix__":
        args.libdir = get_mount_libdir(args.prefix, "/opt/ovis")
    spec["libdir"] = args.libdir
    try:
        cluster = LDMSDCluster.get(spec["name"], create=False, spec = spec)
        cluster.remove()
    except:
        pass
    cluster = LDMSDCluster.get(spec["name"], create=True, spec = spec)
    cluster.start_sshd()

    def _slurm_running():
        # test if slurmd/slurmctld is running on all containers
        for cont in cluster.containers:
            rc, out = cont.exec_run("pgrep slurm")
            if rc:
                return False
        return True

    def _no_slurm_running():
        # test if no slurmd/slurmctld is running
        # and slurmctld (6817) port and slurmd (6818) port are free
        for cont in cluster.containers:
            rc, out = cont.exec_run("pgrep slurm")
            if rc == 0:
                return False
            rc, out = cont.exec_run("grep ':1aa1\\>\\|1aa2\\>' /proc/net/tcp")
            if rc == 0:
                return False
        return True

    log.info("-- Cleanup output --")
    remove_job_out(cluster)

    log.info("-- Test bad plugstack config --")
    # manually start slurmctld + slurmd
    log.info("Starting slurm ...")
    headnode = cluster.get_container("headnode")
    headnode.start_munged()
    headnode.prep_slurm_conf()
    headnode.exec_run("slurmctld")
    for i in range(1, NUM_NODES + 1):
        name = "node-{}".format(i)
        cont = cluster.get_container(name)
        cont.start_munged()
        cont.prep_slurm_conf()
        cont.write_file("/etc/slurm/plugstack.conf",
                "required {libdir}/ovis-ldms/libslurm_notifier.so foo=bar" \
                .format(libdir=args.libdir))
        cont.exec_run("slurmd")
    if not cond_timedwait(_slurm_running, timeout=20):
        log.info("Starting slurm ... timed out")
    else:
        log.info("Starting slurm ... OK")
    time.sleep(10)
    undrain()

    j0 = submit_job(cluster, num_tasks = 4)
    j1 = submit_job(cluster, num_tasks = 4)
    j2 = submit_job(cluster, num_tasks = 4)
    j3 = submit_job(cluster, num_tasks = 4)

    jobs_verified = True
    tasks = [ (j, i) for j in [j0, j1, j2, j3] for i in range(0, 4) ]
    rc = headnode.files_exist(('/db/slurm_env_{j}.{i}.out'.format(j=j, i=i) \
                              for j, i in tasks), timeout=60, interval=1 )
    if rc == False:
        log.info("File checking timed out")
    for j, i in tasks:
        out = headnode.read_file('/db/slurm_env_{jobid}.{procid}.out' \
                             .format(jobid = j, procid=i))
        d = make_dict_from_out(out)
        jobid = int(d.get('SLURM_JOBID', -1))
        taskid = int(d.get('SLURM_PROCID', -1))
        if jobid != j or taskid != i:
            jobs_verified = False
            break
    test.assert_test(60, jobs_verified, 'jobs verified')

    log.info("Killin slurm ...")
    for cont in cluster.containers:
        cont.exec_run("pkill slurm") # this kills both slurmd and slurmctld

    if not cond_timedwait(_no_slurm_running, timeout=20):
        log.info("Killin slurm ... timed out")
    else:
        log.info("Killin slurm ... OK")
    time.sleep(20)
    log.info("-- Start daemons --")
    cluster.start_daemons()
    if not cond_timedwait(_slurm_running, timeout=10):
        log.info("Starting slurm ... timed out")
    else:
        log.info("Starting slurm ... OK")
    time.sleep(10)
    undrain()

    # Test Strategy:
    #
    # 1. Run the test tool ldms_msg_subscriber to listen to stream
    #    data on the stream named "slurm" and writes the events to a
    #    file called 'spank_stream.out'
    #
    # 2. The spank notifier will write respond to the spank events by
    #    writing the job data to an LDMSD Stream called "slurm".
    #
    # 3. Run a job that writes the SLURM environment variables to a
    #    file called 'spank_test.out'
    #
    # 4. The assertions will confirm that the SLURM environment
    #    variables match the data received on the LDMSD Stream
    #


    # If there is no one listening for stream events, the job still runs.
    log.info("-- Submitting job with no stream listener --")
    num_tasks = 2 * CPU_PER_NODE
    jobid = submit_job(cluster, num_tasks = num_tasks)
    time.sleep(15) # enough time for the job to run

    # Read the environment file to confirm that the job ran
    assert_no = 0
    _nodes = []
    rc = headnode.files_exist(('/db/slurm_env_{j}.{p}.out'.format(j=jobid, p=procid) \
                     for procid in range(0, num_tasks)), timeout = 60)
    if rc == False:
        log.info("File checking timed out")
    for procid in range(0, num_tasks): # 8 procs span 2 nodes
        out = headnode.read_file('/db/slurm_env_{jobid}.{procid}.out' \
                             .format(jobid = jobid, procid=procid))
        d = make_dict_from_out(out)
        _nodes.append(d.get('SLURM_TOPOLOGY_ADDR'))

    _node_set = set(_nodes)

    for node in _node_set:
        test.assert_test(assert_no, _nodes.count(node) == CPU_PER_NODE,
                         'job output file created')
        assert_no += 1

    slurmd_containers = [cluster.get_container("node-{}".format(i)) \
                                        for i in range(1, NUM_NODES+1)]

    # start a stream subscriber
    for cont in slurmd_containers:
        rc, out = cont.exec_run("stdbuf -o0 ldms_msg_subscribe -p 20000 "\
                                "-m slurm > /db/spank_stream_{node}.out "\
                                "&".format(node=cont.hostname))
        if rc != 0:
            log.info(out)
            cluster.remove()
            sys.exit(1)

    time.sleep(5) # should be enough for initialization

    log.info("-- Submitting job with listener --")

    ntasks = [1, 2, 4, 8, 27]
    jobinfo_list = []
    _first_assertion = 2
    subscriber_data = {"sub": "data"}
    for n in ntasks:
        jobid = submit_job(cluster, num_tasks = n)
        jobinfo = { "jobid": jobid, "num_tasks": n,
                    "first_assertion": _first_assertion,
                    "subscriber_data": subscriber_data }
        _first_assertion += 5
        jobinfo_list.append(jobinfo)
    time.sleep(20) # enough time for the job to run

    # Parse node events
    node_events = []
    all_events = []
    for cont in slurmd_containers:
        rc = cont.files_exist('/db/spank_stream_{}.out'.format(cont.hostname), timeout=60)
        if rc == False:
            log.info("File checking timed out")
    for cont in slurmd_containers:
        f = '/db/spank_stream_{node}.out'.format(node=cont.hostname)
        cmd = 'grep \'^data: \' {}'.format(f)
        rc, data = cont.exec_run(cmd)
        events = data.split('data:')
        event_list = []
        for event in events:
            if len(event) == 0:
                continue
            decoder = json.JSONDecoder()
            d, pos = decoder.raw_decode(event.strip())
            e = d
            e['hostname'] = cont.hostname # also tag hostname in the event
            event_list.append(e)
            all_events.append(e)
        node_events.append(event_list)

    all_events.sort(key = lambda x: x['timestamp']) # sort by timestamp

    log.info("-- Verifying Events --")
    # Verify jobs
    for jobinfo in jobinfo_list:
        verify_jobinfo(cluster, test, node_events, jobinfo)

    # Search for multi-tenant
    mt_found = False
    for events in node_events:
        if not events:
            continue
        hostname = events[0]['hostname']
        jobev = {} # jobid: last_event
        # only look for `init` and `exit`
        for ev in events:
            jobid = ev['data']['job_id']
            if ev['event'] == 'init':
                if jobev:
                    mt_found = True
                    log.info("job {} multi-tenant with {}" \
                             .format(jobid, jobev.keys()))
                jobev[jobid] = ev
            elif ev['event'] == 'exit':
                # remove
                jobev.pop(jobid)

    test.assert_test(50, mt_found, 'Multi-tenant jobs found')

    # Run a job that crashes the stream listener while the job is running
    headnode.write_file('/db/crash_stream_listener.sh',
                    '#!/bin/bash\n'\
                    'pkill ldms_msg_subscriber\n'\
                    'env | grep SLURM > /db/crash_env_${SLURM_JOBID}.${SLURM_PROCID}.out\n')
    headnode.write_file('/db/spank_job_2.sh',
                    '#!/bin/bash\n'\
                    '#SBATCH -N 2\n'\
                    '#SBATCH --output spank_test.out\n'\
                    '#SBATCH -D /db\n'\
                    'export SUBSCRIBER_DATA=\'{"sub":"data"}\'\n'\
                    'srun /bin/bash /db/crash_stream_listener.sh\n')

    log.info("-- Submitting job that crashes listener --")
    jobid = cluster.sbatch("/db/spank_job_2.sh")
    log.info("  jobid = {0}".format(jobid))
    time.sleep(10) # enough time for the job to run

    assert_no = 51
    rc = headnode.files_exist(('/db/crash_env_{}.{}.out'.format(jobid, procid) \
                        for procid in (0, 1)), timeout=60 )
    if rc == False:
        log.info("File checking timed out")
    for procid in [ 0, 1 ]:
        out = headnode.read_file('/db/crash_env_{0}.{1}.out'.format(jobid, procid))
        d = make_dict_from_out(out)
        test.assert_test(assert_no, 'SLURM_JOB_NAME' in d, 'job output file created')
        assert_no += 1

# see at_exit()
